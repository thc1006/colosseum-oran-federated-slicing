{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovA-JyqbUHce"
   },
   "source": [
    "### 1. 環境設置與資料載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 119175,
     "status": "ok",
     "timestamp": 1751587014421,
     "user": {
      "displayName": "Alex Tsai",
      "userId": "08548353837065676083"
     },
     "user_tz": -480
    },
    "id": "0H-_4xOoTP_O",
    "outputId": "cc1b746c-f050-4ac3-f16f-da0c9608296a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始下載 ColO-RAN 資料集...\n",
      "下載位置: /content/colosseum-oran-coloran-dataset\n",
      "✅ 資料集下載成功！\n",
      "\n",
      "📁 資料集結構預覽：\n",
      "total 68\n",
      "drwxr-xr-x 4 root root  4096 Jul  3 23:55 .\n",
      "drwxr-xr-x 1 root root  4096 Jul  3 23:54 ..\n",
      "-rw-r--r-- 1 root root  1186 Jul  3 23:55 CITATION.cff\n",
      "drwxr-xr-x 8 root root  4096 Jul  3 23:56 .git\n",
      "-rw-r--r-- 1 root root    36 Jul  3 23:55 .gitignore\n",
      "-rw-r--r-- 1 root root 35149 Jul  3 23:55 LICENSE\n",
      "-rw-r--r-- 1 root root  5426 Jul  3 23:55 README.md\n",
      "drwxr-xr-x 5 root root  4096 Jul  3 23:56 rome_static_medium\n",
      "\n",
      "✅ 找到 rome_static_medium 資料夾\n",
      "total 20\n",
      "drwxr-xr-x  5 root root 4096 Jul  3 23:56 .\n",
      "drwxr-xr-x  4 root root 4096 Jul  3 23:55 ..\n",
      "drwxr-xr-x 30 root root 4096 Jul  3 23:56 sched0\n",
      "drwxr-xr-x 30 root root 4096 Jul  3 23:56 sched1\n",
      "drwxr-xr-x 30 root root 4096 Jul  3 23:56 sched2\n",
      "\n",
      "🎯 資料集路徑已設定為: /content/colosseum-oran-coloran-dataset\n"
     ]
    }
   ],
   "source": [
    "# @title 下載 ColO-RAN 資料集到 Colab 檔案空間\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# 檢查並下載資料集\n",
    "dataset_repo_url = \"https://github.com/wineslab/colosseum-oran-coloran-dataset.git\"\n",
    "dataset_local_path = \"/content/colosseum-oran-coloran-dataset\"\n",
    "\n",
    "print(\"開始下載 ColO-RAN 資料集...\")\n",
    "print(f\"下載位置: {dataset_local_path}\")\n",
    "\n",
    "# 如果資料夾已存在，先刪除\n",
    "if os.path.exists(dataset_local_path):\n",
    "    print(\"發現現有資料夾，正在清理...\")\n",
    "    !rm -rf {dataset_local_path}\n",
    "\n",
    "# Git clone 資料集\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        \"git\", \"clone\", dataset_repo_url, dataset_local_path\n",
    "    ], capture_output=True, text=True, timeout=600)\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ 資料集下載成功！\")\n",
    "\n",
    "        # 檢查下載的內容\n",
    "        print(\"\\n📁 資料集結構預覽：\")\n",
    "        !ls -la {dataset_local_path}\n",
    "\n",
    "        # 檢查 rome_static_medium 資料夾\n",
    "        rome_path = f\"{dataset_local_path}/rome_static_medium\"\n",
    "        if os.path.exists(rome_path):\n",
    "            print(f\"\\n✅ 找到 rome_static_medium 資料夾\")\n",
    "            !ls -la {rome_path}\n",
    "        else:\n",
    "            print(f\"\\n❌ 未找到 rome_static_medium 資料夾，列出所有內容：\")\n",
    "            !find {dataset_local_path} -type d -maxdepth 2\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ 下載失敗: {result.stderr}\")\n",
    "\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"❌ 下載超時，請檢查網路連線\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 下載過程中發生錯誤: {e}\")\n",
    "\n",
    "# 設定新的資料集路徑\n",
    "DATASET_PATH = dataset_local_path\n",
    "print(f\"\\n🎯 資料集路徑已設定為: {DATASET_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiiPPUYQT8sx"
   },
   "source": [
    "### 2. 資料集載入與整合函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 429504,
     "status": "ok",
     "timestamp": 1751587443942,
     "user": {
      "displayName": "Alex Tsai",
      "userId": "08548353837065676083"
     },
     "user_tz": -480
    },
    "id": "xzQaeDGnT2fv",
    "outputId": "44f03c11-e77f-40d6-b104-25cc1af07724"
   },
   "outputs": [],
   "source": [
    "# @title 完整版 ColO-RAN 資料處理器（載入全部 588 個組合）\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ColoRANDataProcessorPro:\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.base_stations = [1, 8, 15, 22, 29, 36, 43]\n",
    "        self.scheduling_policies = ['sched0', 'sched1', 'sched2']  # RR, WF, PF\n",
    "\n",
    "        # 全部 28 個訓練配置 - 不再限制記憶體\n",
    "        self.training_configs = [f'tr{i}' for i in range(28)]\n",
    "\n",
    "        # 切片配置定義（完整版）\n",
        self.slice_configs = {\\n",
    "            'tr0': [2, 13, 2], 'tr1': [4, 11, 2], 'tr2': [6, 9, 2], 'tr3': [8, 7, 2],\n",
    "            'tr4': [10, 5, 2], 'tr5': [12, 3, 2], 'tr6': [14, 1, 2], 'tr7': [2, 11, 4],\n",
    "            'tr8': [4, 9, 4], 'tr9': [6, 7, 4], 'tr10': [8, 5, 4], 'tr11': [10, 3, 4],\n",
    "            'tr12': [12, 1, 4], 'tr13': [2, 9, 6], 'tr14': [4, 7, 6], 'tr15': [6, 5, 6],\n",
    "            'tr16': [8, 3, 6], 'tr17': [10, 1, 6], 'tr18': [2, 7, 8], 'tr19': [4, 5, 8],\n",
    "            'tr20': [6, 3, 8], 'tr21': [8, 1, 8], 'tr22': [2, 5, 10], 'tr23': [4, 3, 10],\n",
    "            'tr24': [6, 1, 10], 'tr25': [2, 3, 12], 'tr26': [4, 1, 12], 'tr27': [2, 1, 14]\n",
    "        }\n",
    "\n",
    "        print(f\"🚀 初始化 ColO-RAN Pro 處理器\")\n",
    "        print(f\"📁 資料集路徑: {self.dataset_path}\")\n",
    "        print(f\"🎯 目標配置: 全部 {len(self.training_configs)} 個配置\")\n",
    "        print(f\"📊 預計載入組合數: {len(self.scheduling_policies)} × {len(self.training_configs)} × {len(self.base_stations)} = {len(self.scheduling_policies) * len(self.training_configs) * len(self.base_stations)}\")\n",
    "\n",
    "    def auto_detect_structure(self):\n",
    "        \"\"\"自動偵測資料集結構\"\"\"\n",
    "        print(\"🔍 自動偵測資料集結構...\")\n",
    "\n",
    "        # 可能的路徑結構\n",
    "        possible_paths = [\n",
    "            f\"{self.dataset_path}/rome_static_medium\",\n",
    "            f\"{self.dataset_path}/colosseum-oran-coloran-dataset/rome_static_medium\",\n",
    "            self.dataset_path\n",
    "        ]\n",
    "\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                sched_dirs = [d for d in os.listdir(path) if d.startswith('sched')]\n",
    "                if sched_dirs:\n",
    "                    print(f\"✅ 找到有效結構: {path}\")\n",
    "                    print(f\"📂 發現排程策略: {sched_dirs}\")\n",
    "                    return path\n",
    "\n",
    "        print(\"❌ 未找到標準資料結構，列出可用目錄：\")\n",
    "        if os.path.exists(self.dataset_path):\n",
    "            for item in os.listdir(self.dataset_path):\n",
    "                print(f\"  - {item}\")\n",
    "\n",
    "        return self.dataset_path\n",
    "\n",
    "    def load_all_data_with_glob_pro(self):\n",
    "        \"\"\"使用 glob 載入全部資料（Pro 版本，無記憶體限制）\"\"\"\n",
    "\n",
    "        # 自動偵測資料結構\n",
    "        base_data_path = self.auto_detect_structure()\n",
    "\n",
    "        bs_data_list = []\n",
    "        ue_data_list = []\n",
    "        slice_data_list = []\n",
    "\n",
    "        total_combinations = len(self.scheduling_policies) * len(self.training_configs) * len(self.base_stations)\n",
    "        current = 0\n",
    "        success_count = 0\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🚀 開始載入完整 ColO-RAN 資料集（全部 588 個組合）\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        for sched_policy in self.scheduling_policies:\n",
    "            for training_config in self.training_configs:\n",
    "                print(f\"\\n📋 處理配置: {sched_policy}/{training_config}\")\n",
    "\n",
    "                # 建構搜尋路徑\n",
    "                search_patterns = {\n",
    "                    'bs': f\"{base_data_path}/{sched_policy}/{training_config}/exp*/bs*/bs*.csv\",\n",
    "                    'ue': f\"{base_data_path}/{sched_policy}/{training_config}/exp*/bs*/ue*.csv\",\n",
    "                    'slice': f\"{base_data_path}/{sched_policy}/{training_config}/exp*/bs*/slices_bs*/*_metrics.csv\"\n",
    "                }\n",
    "\n",
    "                # 載入 BS 資料\n",
    "                bs_files = glob.glob(search_patterns['bs'])\n",
    "                print(f\"  📊 BS 檔案: {len(bs_files)} 個\")\n",
    "\n",
    "                for bs_file in bs_files:\n",
    "                    current += 1\n",
    "                    try:\n",
    "                        df = pd.read_csv(bs_file)\n",
    "\n",
    "                        # 解析路徑資訊\n",
    "                        path_parts = bs_file.split('/')\n",
    "                        exp_folder = next((p for p in path_parts if p.startswith('exp')), 'exp1')\n",
    "                        bs_folder = next((p for p in path_parts if p.startswith('bs') and not p.endswith('.csv')), 'bs1')\n",
    "\n",
    "                        bs_id = int(bs_folder.replace('bs', ''))\n",
    "                        exp_id = int(exp_folder.replace('exp', ''))\n",
    "\n",
    "                        # 添加 metadata\n",
    "                        df['bs_id'] = bs_id\n",
    "                        df['exp_id'] = exp_id\n",
    "                        df['sched_policy'] = sched_policy\n",
    "                        df['training_config'] = training_config\n",
    "                        df['file_path'] = bs_file\n",
    "\n",
    "                        bs_data_list.append(df)\n",
    "                        success_count += 1\n",
    "\n",
    "                        if current % 50 == 0:\n",
    "                            print(f\"    ⏳ 進度: {current}/{total_combinations} ({current/total_combinations*100:.1f}%)\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ❌ BS 檔案載入失敗 {bs_file}: {e}\")\n",
    "\n",
    "                # 載入 UE 資料\n",
    "                ue_files = glob.glob(search_patterns['ue'])\n",
    "                print(f\"  📱 UE 檔案: {len(ue_files)} 個\")\n",
    "\n",
    "                for ue_file in ue_files:\n",
    "                    try:\n",
    "                        df = pd.read_csv(ue_file)\n",
    "\n",
    "                        # 解析路徑資訊\n",
    "                        path_parts = ue_file.split('/')\n",
    "                        exp_folder = next((p for p in path_parts if p.startswith('exp')), 'exp1')\n",
    "                        bs_folder = next((p for p in path_parts if p.startswith('bs') and not p.endswith('.csv')), 'bs1')\n",
    "                        ue_file_name = os.path.basename(ue_file)\n",
    "\n",
    "                        bs_id = int(bs_folder.replace('bs', ''))\n",
    "                        exp_id = int(exp_folder.replace('exp', ''))\n",
    "                        ue_id = int(ue_file_name.replace('ue', '').replace('.csv', ''))\n",
    "\n",
    "                        # 添加 metadata\n",
    "                        df['bs_id'] = bs_id\n",
    "                        df['exp_id'] = exp_id\n",
    "                        df['ue_id'] = ue_id\n",
    "                        df['sched_policy'] = sched_policy\n",
    "                        df['training_config'] = training_config\n",
    "                        df['file_path'] = ue_file\n",
    "\n",
    "                        ue_data_list.append(df)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ❌ UE 檔案載入失敗 {ue_file}: {e}\")\n",
    "\n",
    "                # 載入 Slice 資料\n",
    "                slice_files = glob.glob(search_patterns['slice'])\n",
    "                print(f\"  🍰 Slice 檔案: {len(slice_files)} 個\")\n",
    "\n",
    "                for slice_file in slice_files:\n",
    "                    try:\n",
    "                        df = pd.read_csv(slice_file)\n",
    "\n",
    "                        # 解析路徑資訊\n",
    "                        path_parts = slice_file.split('/')\n",
    "                        exp_folder = next((p for p in path_parts if p.startswith('exp')), 'exp1')\n",
    "                        bs_folder = next((p for p in path_parts if p.startswith('bs') and 'slices' not in p), 'bs1')\n",
    "                        slice_file_name = os.path.basename(slice_file)\n",
    "\n",
    "                        bs_id = int(bs_folder.replace('bs', ''))\n",
    "                        exp_id = int(exp_folder.replace('exp', ''))\n",
    "                        imsi = slice_file_name.replace('_metrics.csv', '')\n",
    "\n",
    "                        # 添加 metadata\n",
    "                        df['bs_id'] = bs_id\n",
    "                        df['exp_id'] = exp_id\n",
    "                        df['imsi'] = imsi\n",
    "                        df['sched_policy'] = sched_policy\n",
    "                        df['training_config'] = training_config\n",
    "                        df['file_path'] = slice_file\n",
    "\n",
    "                        slice_data_list.append(df)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ❌ Slice 檔案載入失敗 {slice_file}: {e}\")\n",
    "\n",
    "        # 合併所有資料\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🔗 合併資料中...\")\n",
    "\n",
    "        combined_bs_data = pd.concat(bs_data_list, ignore_index=True) if bs_data_list else None\n",
    "        combined_ue_data = pd.concat(ue_data_list, ignore_index=True) if ue_data_list else None\n",
    "        combined_slice_data = pd.concat(slice_data_list, ignore_index=True) if slice_data_list else None\n",
    "\n",
    "        # 記憶體使用情況\n",
    "        def get_memory_usage(df, name):\n",
    "            if df is not None:\n",
    "                memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "                return f\"{name}: {len(df):,} 筆記錄, {memory_mb:.1f} MB\"\n",
    "            return f\"{name}: 0 筆記錄\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"📊 載入完成統計\")\n",
    "        print(\"=\"*80)\n",
    "        print(get_memory_usage(combined_bs_data, \"基站資料\"))\n",
    "        print(get_memory_usage(combined_ue_data, \"UE資料\"))\n",
    "        print(get_memory_usage(combined_slice_data, \"切片資料\"))\n",
    "        print(f\"✅ 成功載入檔案數: {success_count}\")\n",
    "        print(f\"🎯 載入成功率: {success_count/total_combinations*100:.1f}%\")\n",
    "\n",
    "        return combined_bs_data, combined_ue_data, combined_slice_data\n",
    "\n",
    "# 執行完整載入\n",
    "processor_pro = ColoRANDataProcessorPro(DATASET_PATH)\n",
    "bs_data_full, ue_data_full, slice_data_full = processor_pro.load_all_data_with_glob_pro()\n",
    "\n",
    "# 顯示載入結果摘要\n",
    "print(\"\\n\" + \"🎉\" + \"=\"*78 + \"🎉\")\n",
    "print(\"ColO-RAN 完整資料集載入完成！\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if bs_data_full is not None:\n",
    "    print(f\"📊 基站資料: {len(bs_data_full):,} 筆記錄\")\n",
    "    print(f\"   排程策略分佈: {bs_data_full['sched_policy'].value_counts().to_dict()}\")\n",
    "    print(f\"   訓練配置數量: {bs_data_full['training_config'].nunique()}\")\n",
    "    print(f\"   基站數量: {bs_data_full['bs_id'].nunique()}\")\n",
    "\n",
    "if ue_data_full is not None:\n",
    "    print(f\"📱 UE資料: {len(ue_data_full):,} 筆記錄\")\n",
    "    print(f\"   UE設備數量: {ue_data_full['ue_id'].nunique()}\")\n",
    "\n",
    "if slice_data_full is not None:\n",
    "    print(f\"🍰 切片資料: {len(slice_data_full):,} 筆記錄\")\n",
    "    print(f\"   IMSI數量: {slice_data_full['imsi'].nunique()}\")\n",
    "\n",
    "print(\"\\n🚀 資料集已準備完成，可進行後續分析！\")\n",
    "\n",
    "# ===== 在 Cell 2 最後加入以下程式碼 =====\n",
    "\n",
    "# 保存原始載入的資料為 parquet 檔案\n",
    "def save_raw_data_to_parquet():\n",
    "    \"\"\"將載入的原始資料保存為 parquet 檔案\"\"\"\n",
    "    print(\"\\n\" + \"💾\" + \"=\"*78 + \"💾\")\n",
    "    print(\"保存原始資料為 Parquet 檔案\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    saved_files = []\n",
    "\n",
    "    # 保存基站資料\n",
    "    if bs_data_full is not None and len(bs_data_full) > 0:\n",
    "        bs_filename = 'raw_bs_data.parquet'\n",
    "        bs_data_full.to_parquet(bs_filename, compression='snappy', index=False)\n",
    "        file_size = os.path.getsize(bs_filename) / 1024 / 1024\n",
    "        print(f\"✅ 基站資料已保存: {bs_filename} ({file_size:.1f} MB)\")\n",
    "        saved_files.append(bs_filename)\n",
    "\n",
    "    # 保存 UE 資料\n",
    "    if ue_data_full is not None and len(ue_data_full) > 0:\n",
    "        ue_filename = 'raw_ue_data.parquet'\n",
    "        ue_data_full.to_parquet(ue_filename, compression='snappy', index=False)\n",
    "        file_size = os.path.getsize(ue_filename) / 1024 / 1024\n",
    "        print(f\"✅ UE資料已保存: {ue_filename} ({file_size:.1f} MB)\")\n",
    "        saved_files.append(ue_filename)\n",
    "\n",
    "    # 保存切片資料\n",
    "    if slice_data_full is not None and len(slice_data_full) > 0:\n",
    "        slice_filename = 'raw_slice_data.parquet'\n",
    "        slice_data_full.to_parquet(slice_filename, compression='snappy', index=False)\n",
    "        file_size = os.path.getsize(slice_filename) / 1024 / 1024\n",
    "        print(f\"✅ 切片資料已保存: {slice_filename} ({file_size:.1f} MB)\")\n",
    "        saved_files.append(slice_filename)\n",
    "\n",
    "    # 保存切片配置資訊\n",
    "    import json\n",
    "    config_filename = 'slice_configs.json'\n",
    "    with open(config_filename, 'w') as f:\n",
    "        json.dump(processor_pro.slice_configs, f, indent=2)\n",
    "    print(f\"✅ 切片配置已保存: {config_filename}\")\n",
    "    saved_files.append(config_filename)\n",
    "\n",
    "    print(f\"\\n🎉 所有原始資料已成功保存！\")\n",
    "    print(f\"📁 保存檔案清單: {saved_files}\")\n",
    "    print(f\"💡 下次可直接從 Cell 3 開始執行，自動載入這些檔案\")\n",
    "\n",
    "    return saved_files\n",
    "\n",
    "# 執行保存\n",
    "saved_files = save_raw_data_to_parquet()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vFDUl3XT9-7"
   },
   "source": [
    "### 3. 資料前處理與特徵工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 135298,
     "status": "ok",
     "timestamp": 1751587609316,
     "user": {
      "displayName": "Alex Tsai",
      "userId": "08548353837065676083"
     },
     "user_tz": -480
    },
    "id": "GI-AdULf0DB3",
    "outputId": "34ff872c-a724-409c-e409-df91dcdc5cee"
   },
   "outputs": [],
   "source": [
    "# @title 記憶體優化特徵工程處理器（含自動檢查點讀取與分批處理）\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_raw_data_if_exists():\n",
    "    \"\"\"檢查並載入已保存的原始資料\"\"\"\n",
    "    print(\"🔍 檢查是否存在已保存的原始資料...\")\n",
    "\n",
    "    # 檢查必要檔案是否存在\n",
    "    required_files = [\n",
    "        'raw_bs_data.parquet',\n",
    "        'raw_ue_data.parquet',\n",
    "        'raw_slice_data.parquet',\n",
    "        'slice_configs.json'\n",
    "    ]\n",
    "\n",
    "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "\n",
    "    if missing_files:\n",
    "        print(f\"❌ 缺少檔案: {missing_files}\")\n",
    "        print(\"💡 請先執行 Cell 2 載入原始資料\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(\"✅ 找到所有必要檔案，開始載入...\")\n",
    "\n",
    "    try:\n",
    "        # 載入資料\n",
    "        bs_data = pd.read_parquet('raw_bs_data.parquet')\n",
    "        ue_data = pd.read_parquet('raw_ue_data.parquet')\n",
    "        slice_data = pd.read_parquet('raw_slice_data.parquet')\n",
    "\n",
    "        # 載入配置\n",
    "        with open('slice_configs.json', 'r') as f:\n",
    "            slice_configs = json.load(f)\n",
    "\n",
    "        print(f\"✅ 資料載入完成！\")\n",
    "        print(f\"   📊 基站資料: {len(bs_data):,} 筆記錄\")\n",
    "        print(f\"   📱 UE資料: {len(ue_data):,} 筆記錄\")\n",
    "        print(f\"   🍰 切片資料: {len(slice_data):,} 筆記錄\")\n",
    "        print(f\"   ⚙️ 切片配置: {len(slice_configs)} 個\")\n",
    "\n",
    "        return bs_data, ue_data, slice_data, slice_configs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 載入資料時發生錯誤: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "class MemoryOptimizedNetworkSliceProcessor:\n",
    "    def __init__(self, slice_configs, batch_size=100000):\n",
    "        self.slice_configs = slice_configs\n",
    "        self.batch_size = batch_size\n",
    "        self.processed_batches = []\n",
    "\n",
    "        print(f\"🔧 初始化記憶體優化處理器\")\n",
    "        print(f\"📦 批次大小: {self.batch_size:,} 筆記錄\")\n",
    "\n",
    "    def process_data_in_batches(self, slice_data):\n",
    "        \"\"\"分批處理大型資料集\"\"\"\n",
    "        print(f\"🚀 開始分批處理，總記錄數: {len(slice_data):,}\")\n",
    "\n",
    "        total_rows = len(slice_data)\n",
    "        num_batches = (total_rows + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        processed_results = []\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * self.batch_size\n",
    "            end_idx = min((batch_idx + 1) * self.batch_size, total_rows)\n",
    "\n",
    "            print(f\"  📦 處理批次 {batch_idx + 1}/{num_batches} ({start_idx:,}-{end_idx:,})\")\n",
    "\n",
    "            # 取得當前批次資料\n",
    "            batch_data = slice_data.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "            # 處理當前批次\n",
    "            processed_batch = self._process_single_batch(batch_data)\n",
    "\n",
    "            if processed_batch is not None:\n",
    "                processed_results.append(processed_batch)\n",
    "                print(f\"    ✅ 批次處理完成: {len(processed_batch):,} 筆記錄\")\n",
    "\n",
    "            # 立即釋放記憶體\n",
    "            del batch_data, processed_batch\n",
    "            gc.collect()\n",
    "\n",
    "            # 顯示記憶體使用狀況\n",
    "            if (batch_idx + 1) % 5 == 0:\n",
    "                memory_info = self._get_memory_info()\n",
    "                print(f\"    💾 記憶體使用: {memory_info}\")\n",
    "\n",
    "        # 合併所有批次結果\n",
    "        if processed_results:\n",
    "            print(\"🔗 合併所有批次結果...\")\n",
    "            final_result = pd.concat(processed_results, ignore_index=True)\n",
    "\n",
    "            # 清理中間結果\n",
    "            del processed_results\n",
    "            gc.collect()\n",
    "\n",
    "            return final_result\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _process_single_batch(self, df):\n",
    "        \"\"\"處理單一批次的資料\"\"\"\n",
    "        try:\n",
    "            # 1. 時間特徵（向量化）\n",
    "            if 'Timestamp' in df.columns:\n",
    "                timestamps = pd.to_datetime(df['Timestamp'], unit='ms', errors='coerce')\n",
    "                df['hour'] = timestamps.dt.hour\n",
    "                df['minute'] = timestamps.dt.minute\n",
    "                df['day_of_week'] = timestamps.dt.dayofweek\n",
    "                # 不保留完整的 timestamp 欄位以節省記憶體\n",
    "                del timestamps\n",
    "\n",
    "            # 2. 排程策略編碼（向量化）\n",
    "            sched_mapping = {'sched0': 0, 'sched1': 1, 'sched2': 2}\n",
    "            df['sched_policy_num'] = df['sched_policy'].map(sched_mapping)\n",
    "\n",
    "            # 3. RBG 配置（向量化優化）\n",
    "            df['allocated_rbgs'] = self._vectorized_rbg_allocation(df)\n",
    "\n",
    "            # 4. 資源利用率（向量化）\n",
    "            df['sum_requested_prbs'] = df.get('sum_requested_prbs', 0).fillna(0)\n",
    "            df['sum_granted_prbs'] = df.get('sum_granted_prbs', 0).fillna(0)\n",
    "            df['prb_utilization'] = np.where(\n",
    "                df['sum_requested_prbs'] > 0,\n",
    "                df['sum_granted_prbs'] / df['sum_requested_prbs'],\n",
    "                0\n",
    "            ).clip(0, 1)\n",
    "\n",
    "            # 5. 吞吐量效率（向量化）\n",
    "            throughput_col = 'tx_brate downlink [Mbps]'\n",
    "            if throughput_col in df.columns:\n",
    "                df['throughput_efficiency'] = np.where(\n",
    "                    df['sum_granted_prbs'] > 0,\n",
    "                    df[throughput_col].fillna(0) / df['sum_granted_prbs'],\n",
    "                    0\n",
    "                )\n",
    "            else:\n",
    "                df['throughput_efficiency'] = 0.0\n",
    "\n",
    "            # 6. QoS 評分（向量化）\n",
    "            df['qos_score'] = self._calculate_qos_score_vectorized(df)\n",
    "\n",
    "            # 7. 網路負載\n",
    "            df['num_ues'] = df.get('num_ues', 1).fillna(1)\n",
    "            df['network_load'] = df['num_ues'] / 42.0\n",
    "\n",
    "            # 8. 綜合效率指標\n",
    "            df['allocation_efficiency'] = (\n",
    "                0.5 * df['throughput_efficiency'] +\n",
    "                0.3 * df['qos_score'] +\n",
    "                0.2 * df['prb_utilization']\n",
    "            ).clip(0, 1)\n",
    "\n",
    "            # 9. 選擇需要的欄位，移除不必要的欄位\n",
    "            required_columns = [\n",
    "                'num_ues', 'slice_id', 'sched_policy_num', 'allocated_rbgs',\n",
    "                'bs_id', 'exp_id', 'sum_requested_prbs', 'sum_granted_prbs',\n",
    "                'prb_utilization', 'throughput_efficiency', 'qos_score',\n",
    "                'network_load', 'hour', 'minute', 'day_of_week',\n",
    "                'allocation_efficiency', 'sched_policy', 'training_config'\n",
    "            ]\n",
    "\n",
    "            available_columns = [col for col in required_columns if col in df.columns]\n",
    "            df_result = df[available_columns].copy()\n",
    "\n",
    "            # 清理異常值\n",
    "            df_result = df_result.dropna(subset=['allocation_efficiency'])\n",
    "\n",
    "            return df_result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    ❌ 批次處理失敗: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _vectorized_rbg_allocation(self, df):\n",
    "        \"\"\"向量化的 RBG 配置計算\"\"\"\n",
    "        # 創建配置對應字典\n",
    "        config_map = {}\n",
    "        for config_name, rbg_list in self.slice_configs.items():\n",
    "            for slice_id in range(len(rbg_list)):\n",
    "                config_map[(config_name, slice_id)] = rbg_list[slice_id]\n",
    "\n",
    "        # 向量化映射\n",
    "        allocation_keys = list(zip(df['training_config'], df.get('slice_id', 0).fillna(0)))\n",
    "        return pd.Series(allocation_keys).map(config_map).fillna(0).values\n",
    "\n",
    "    def _calculate_qos_score_vectorized(self, df):\n",
    "        \"\"\"向量化的 QoS 評分計算\"\"\"\n",
    "        dl_error_col = 'tx_errors downlink (%)'\n",
    "        ul_error_col = 'rx_errors uplink (%)'\n",
    "        cqi_col = 'dl_cqi'\n",
    "\n",
    "        dl_score = 0.5  # 預設值\n",
    "        ul_score = 0.5  # 預設值\n",
    "        cqi_score = 0.5  # 預設值\n",
    "\n",
    "        if dl_error_col in df.columns:\n",
    "            dl_score = (100 - df[dl_error_col].fillna(50)) / 100\n",
    "\n",
    "        if ul_error_col in df.columns:\n",
    "            ul_score = (100 - df[ul_error_col].fillna(50)) / 100\n",
    "\n",
    "        if cqi_col in df.columns:\n",
    "            cqi_score = df[cqi_col].fillna(7.5) / 15\n",
    "\n",
    "        return (0.4 * dl_score + 0.3 * ul_score + 0.3 * cqi_score).clip(0, 1)\n",
    "\n",
    "    def _get_memory_info(self):\n",
    "        \"\"\"取得記憶體使用資訊\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process()\n",
    "            memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "            return f\"{memory_mb:.1f} MB\"\n",
    "        except:\n",
    "            return \"無法取得\"\n",
    "\n",
    "def optimize_datatypes(df):\n",
    "    \"\"\"最佳化資料型別以節省記憶體\"\"\"\n",
    "    print(\"🔧 最佳化資料型別...\")\n",
    "\n",
    "    initial_memory = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "\n",
    "    # 整數型別最佳化\n",
    "    int_cols = df.select_dtypes(include=['int64']).columns\n",
    "    for col in int_cols:\n",
    "        col_min, col_max = df[col].min(), df[col].max()\n",
    "        if col_min >= 0 and col_max < 255:\n",
    "            df[col] = df[col].astype('uint8')\n",
    "        elif col_min >= 0 and col_max < 65535:\n",
    "            df[col] = df[col].astype('uint16')\n",
    "        elif col_min >= -128 and col_max < 127:\n",
    "            df[col] = df[col].astype('int8')\n",
    "        elif col_min >= -32768 and col_max < 32767:\n",
    "            df[col] = df[col].astype('int16')\n",
    "\n",
    "    # 浮點數型別最佳化\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    for col in float_cols:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "    # 類別型資料\n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in object_cols:\n",
    "        if df[col].nunique() / len(df) < 0.5:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    final_memory = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    print(f\"  💾 記憶體最佳化: {initial_memory:.1f} MB → {final_memory:.1f} MB\")\n",
    "    print(f\"  📉 節省: {((initial_memory - final_memory) / initial_memory * 100):.1f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_processed_data_to_parquet(processed_data, feature_names):\n",
    "    \"\"\"保存處理後的資料為 Parquet 格式\"\"\"\n",
    "    if processed_data is None or len(processed_data) == 0:\n",
    "        print(\"❌ 沒有資料可儲存\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n💾 保存處理後的資料...\")\n",
    "\n",
    "    # 最佳化資料型別\n",
    "    optimized_data = optimize_datatypes(processed_data.copy())\n",
    "\n",
    "    # 保存為 Parquet\n",
    "    output_filename = 'coloran_processed_features.parquet'\n",
    "    optimized_data.to_parquet(\n",
    "        output_filename,\n",
    "        compression='snappy',\n",
    "        index=False,\n",
    "        engine='pyarrow'\n",
    "    )\n",
    "\n",
    "    # 計算檔案大小\n",
    "    file_size = os.path.getsize(output_filename) / 1024 / 1024\n",
    "    memory_size = optimized_data.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "\n",
    "    print(f\"✅ 特徵資料已保存: {output_filename}\")\n",
    "    print(f\"📊 記錄數量: {len(optimized_data):,}\")\n",
    "    print(f\"📋 特徵數量: {len(feature_names)}\")\n",
    "    print(f\"💾 檔案大小: {file_size:.1f} MB\")\n",
    "    print(f\"🗜️ 壓縮效率: {memory_size/file_size:.1f}x\")\n",
    "\n",
    "    # 保存特徵名稱和元資料\n",
    "    feature_info = {\n",
    "        'feature_names': feature_names,\n",
    "        'total_records': len(optimized_data),\n",
    "        'processing_date': datetime.now().isoformat(),\n",
    "        'file_size_mb': file_size,\n",
    "        'compression_ratio': memory_size/file_size\n",
    "    }\n",
    "\n",
    "    with open('feature_metadata.json', 'w') as f:\n",
    "        json.dump(feature_info, f, indent=2)\n",
    "\n",
    "    print(f\"📋 特徵元資料已保存: feature_metadata.json\")\n",
    "\n",
    "    # 清理記憶體\n",
    "    del optimized_data\n",
    "    gc.collect()\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "def main_processing_pipeline():\n",
    "    \"\"\"主要處理管道\"\"\"\n",
    "    print(\"🚀 啟動記憶體優化特徵工程管道\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. 載入原始資料\n",
    "    bs_data_loaded, ue_data_loaded, slice_data_loaded, slice_configs_loaded = load_raw_data_if_exists()\n",
    "\n",
    "    # 如果無法載入保存的檔案，則嘗試使用 Cell 2 的變數\n",
    "    if slice_data_loaded is None:\n",
    "        print(\"⚠️ 嘗試使用 Cell 2 的記憶體變數...\")\n",
    "        if 'bs_data_full' in globals():\n",
    "            bs_data_loaded = bs_data_full\n",
    "            ue_data_loaded = ue_data_full\n",
    "            slice_data_loaded = slice_data_full\n",
    "            slice_configs_loaded = processor_pro.slice_configs\n",
    "            print(\"✅ 成功使用 Cell 2 的記憶體資料\")\n",
    "        else:\n",
    "            print(\"❌ 請先執行 Cell 2 載入資料\")\n",
    "            return None, None\n",
    "\n",
    "    if slice_data_loaded is None or len(slice_data_loaded) == 0:\n",
    "        print(\"❌ 無可用的切片資料\")\n",
    "        return None, None\n",
    "\n",
    "    # 2. 初始化記憶體優化處理器\n",
    "    print(f\"\\n🔧 初始化處理器...\")\n",
    "    processor = MemoryOptimizedNetworkSliceProcessor(\n",
    "        slice_configs_loaded,\n",
    "        batch_size=75000  # 根據 83.5 GB RAM 調整的批次大小\n",
    "    )\n",
    "\n",
    "    # 3. 分批處理資料\n",
    "    print(f\"\\n📊 開始分批特徵工程...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    processed_data = processor.process_data_in_batches(slice_data_loaded)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "    if processed_data is None:\n",
    "        print(\"❌ 特徵工程處理失敗\")\n",
    "        return None, None\n",
    "\n",
    "    # 4. 立即清理原始資料以釋放記憶體\n",
    "    del bs_data_loaded, ue_data_loaded, slice_data_loaded\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"✅ 特徵工程完成!\")\n",
    "    print(f\"⏱️ 處理時間: {processing_time:.2f} 秒\")\n",
    "    print(f\"📊 最終記錄數: {len(processed_data):,}\")\n",
    "\n",
    "    # 5. 建立特徵列表\n",
    "    feature_columns = [\n",
    "        'num_ues', 'slice_id', 'sched_policy_num', 'allocated_rbgs',\n",
    "        'bs_id', 'exp_id', 'sum_requested_prbs', 'sum_granted_prbs',\n",
    "        'prb_utilization', 'throughput_efficiency', 'qos_score',\n",
    "        'network_load', 'hour', 'minute', 'day_of_week'\n",
    "    ]\n",
    "\n",
    "    available_features = [f for f in feature_columns if f in processed_data.columns]\n",
    "\n",
    "    print(f\"\\n📋 可用特徵 ({len(available_features)} 個):\")\n",
    "    for i, feature in enumerate(available_features, 1):\n",
    "        print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "    # 6. 保存處理後的資料\n",
    "    saved_file = save_processed_data_to_parquet(processed_data, available_features)\n",
    "\n",
    "    # 7. 顯示統計資訊\n",
    "    if 'allocation_efficiency' in processed_data.columns:\n",
    "        print(f\"\\n📈 目標變數統計:\")\n",
    "        stats = processed_data['allocation_efficiency'].describe()\n",
    "        print(f\"   平均: {stats['mean']:.4f}\")\n",
    "        print(f\"   標準差: {stats['std']:.4f}\")\n",
    "        print(f\"   範圍: {stats['min']:.4f} - {stats['max']:.4f}\")\n",
    "\n",
    "    print(f\"\\n🎉 Cell 3 執行完成！\")\n",
    "    print(f\"📁 輸出檔案: {saved_file}\")\n",
    "    print(f\"💡 下一步: 執行 Cell 4 進行機器學習訓練\")\n",
    "\n",
    "    return processed_data, available_features\n",
    "\n",
    "# 執行主要處理管道\n",
    "processed_data_full, feature_names_full = main_processing_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPdmCUyZS2ErJIGfuOhpDYg",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
