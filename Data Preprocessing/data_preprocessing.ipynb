{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovA-JyqbUHce"
   },
   "source": [
    "### 1. ç’°å¢ƒè¨­ç½®èˆ‡è³‡æ–™è¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 119175,
     "status": "ok",
     "timestamp": 1751587014421,
     "user": {
      "displayName": "Alex Tsai",
      "userId": "08548353837065676083"
     },
     "user_tz": -480
    },
    "id": "0H-_4xOoTP_O",
    "outputId": "cc1b746c-f050-4ac3-f16f-da0c9608296a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é–‹å§‹ä¸‹è¼‰ ColO-RAN è³‡æ–™é›†...\n",
      "ä¸‹è¼‰ä½ç½®: /content/colosseum-oran-coloran-dataset\n",
      "âœ… è³‡æ–™é›†ä¸‹è¼‰æˆåŠŸï¼\n",
      "\n",
      "ðŸ“ è³‡æ–™é›†çµæ§‹é è¦½ï¼š\n",
      "total 68\n",
      "drwxr-xr-x 4 root root  4096 Jul  3 23:55 .\n",
      "drwxr-xr-x 1 root root  4096 Jul  3 23:54 ..\n",
      "-rw-r--r-- 1 root root  1186 Jul  3 23:55 CITATION.cff\n",
      "drwxr-xr-x 8 root root  4096 Jul  3 23:56 .git\n",
      "-rw-r--r-- 1 root root    36 Jul  3 23:55 .gitignore\n",
      "-rw-r--r-- 1 root root 35149 Jul  3 23:55 LICENSE\n",
      "-rw-r--r-- 1 root root  5426 Jul  3 23:55 README.md\n",
      "drwxr-xr-x 5 root root  4096 Jul  3 23:56 rome_static_medium\n",
      "\n",
      "âœ… æ‰¾åˆ° rome_static_medium è³‡æ–™å¤¾\n",
      "total 20\n",
      "drwxr-xr-x  5 root root 4096 Jul  3 23:56 .\n",
      "drwxr-xr-x  4 root root 4096 Jul  3 23:55 ..\n",
      "drwxr-xr-x 30 root root 4096 Jul  3 23:56 sched0\n",
      "drwxr-xr-x 30 root root 4096 Jul  3 23:56 sched1\n",
      "drwxr-xr-x 30 root root 4096 Jul  3 23:56 sched2\n",
      "\n",
      "ðŸŽ¯ è³‡æ–™é›†è·¯å¾‘å·²è¨­å®šç‚º: /content/colosseum-oran-coloran-dataset\n"
     ]
    }
   ],
   "source": [
    "# @title ä¸‹è¼‰ ColO-RAN è³‡æ–™é›†åˆ° Colab æª”æ¡ˆç©ºé–“\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# æª¢æŸ¥ä¸¦ä¸‹è¼‰è³‡æ–™é›†\n",
    "dataset_repo_url = \"https://github.com/wineslab/colosseum-oran-coloran-dataset.git\"\n",
    "dataset_local_path = \"/content/colosseum-oran-coloran-dataset\"\n",
    "\n",
    "print(\"é–‹å§‹ä¸‹è¼‰ ColO-RAN è³‡æ–™é›†...\")\n",
    "print(f\"ä¸‹è¼‰ä½ç½®: {dataset_local_path}\")\n",
    "\n",
    "# å¦‚æžœè³‡æ–™å¤¾å·²å­˜åœ¨ï¼Œå…ˆåˆªé™¤\n",
    "if os.path.exists(dataset_local_path):\n",
    "    print(\"ç™¼ç¾ç¾æœ‰è³‡æ–™å¤¾ï¼Œæ­£åœ¨æ¸…ç†...\")\n",
    "    !rm -rf {dataset_local_path}\n",
    "\n",
    "# Git clone è³‡æ–™é›†\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        \"git\", \"clone\", dataset_repo_url, dataset_local_path\n",
    "    ], capture_output=True, text=True, timeout=600)\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… è³‡æ–™é›†ä¸‹è¼‰æˆåŠŸï¼\")\n",
    "\n",
    "        # æª¢æŸ¥ä¸‹è¼‰çš„å…§å®¹\n",
    "        print(\"\\nðŸ“ è³‡æ–™é›†çµæ§‹é è¦½ï¼š\")\n",
    "        !ls -la {dataset_local_path}\n",
    "\n",
    "        # æª¢æŸ¥ rome_static_medium è³‡æ–™å¤¾\n",
    "        rome_path = f\"{dataset_local_path}/rome_static_medium\"\n",
    "        if os.path.exists(rome_path):\n",
    "            print(f\"\\nâœ… æ‰¾åˆ° rome_static_medium è³‡æ–™å¤¾\")\n",
    "            !ls -la {rome_path}\n",
    "        else:\n",
    "            print(f\"\\nâŒ æœªæ‰¾åˆ° rome_static_medium è³‡æ–™å¤¾ï¼Œåˆ—å‡ºæ‰€æœ‰å…§å®¹ï¼š\")\n",
    "            !find {dataset_local_path} -type d -maxdepth 2\n",
    "\n",
    "    else:\n",
    "        print(f\"âŒ ä¸‹è¼‰å¤±æ•—: {result.stderr}\")\n",
    "\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"âŒ ä¸‹è¼‰è¶…æ™‚ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·š\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä¸‹è¼‰éŽç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "\n",
    "# è¨­å®šæ–°çš„è³‡æ–™é›†è·¯å¾‘\n",
    "DATASET_PATH = dataset_local_path\n",
    "print(f\"\\nðŸŽ¯ è³‡æ–™é›†è·¯å¾‘å·²è¨­å®šç‚º: {DATASET_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiiPPUYQT8sx"
   },
   "source": [
    "### 2. è³‡æ–™é›†è¼‰å…¥èˆ‡æ•´åˆå‡½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 429504,
     "status": "ok",
     "timestamp": 1751587443942,
     "user": {
      "displayName": "Alex Tsai",
      "userId": "08548353837065676083"
     },
     "user_tz": -480
    },
    "id": "xzQaeDGnT2fv",
    "outputId": "44f03c11-e77f-40d6-b104-25cc1af07724"
   },
   "outputs": [],
   "source": [
    "# @title å®Œæ•´ç‰ˆ ColO-RAN è³‡æ–™è™•ç†å™¨ï¼ˆè¼‰å…¥å…¨éƒ¨ 588 å€‹çµ„åˆï¼‰\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ColoRANDataProcessorPro:\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.base_stations = [1, 8, 15, 22, 29, 36, 43]\n",
    "        self.scheduling_policies = ['sched0', 'sched1', 'sched2']  # RR, WF, PF\n",
    "\n",
    "        # å…¨éƒ¨ 28 å€‹è¨“ç·´é…ç½® - ä¸å†é™åˆ¶è¨˜æ†¶é«”\n",
    "        self.training_configs = [f'tr{i}' for i in range(28)]\n",
    "\n",
    "        # åˆ‡ç‰‡é…ç½®å®šç¾©ï¼ˆå®Œæ•´ç‰ˆï¼‰\n",
        self.slice_configs = {\\n",
    "            'tr0': [2, 13, 2], 'tr1': [4, 11, 2], 'tr2': [6, 9, 2], 'tr3': [8, 7, 2],\n",
    "            'tr4': [10, 5, 2], 'tr5': [12, 3, 2], 'tr6': [14, 1, 2], 'tr7': [2, 11, 4],\n",
    "            'tr8': [4, 9, 4], 'tr9': [6, 7, 4], 'tr10': [8, 5, 4], 'tr11': [10, 3, 4],\n",
    "            'tr12': [12, 1, 4], 'tr13': [2, 9, 6], 'tr14': [4, 7, 6], 'tr15': [6, 5, 6],\n",
    "            'tr16': [8, 3, 6], 'tr17': [10, 1, 6], 'tr18': [2, 7, 8], 'tr19': [4, 5, 8],\n",
    "            'tr20': [6, 3, 8], 'tr21': [8, 1, 8], 'tr22': [2, 5, 10], 'tr23': [4, 3, 10],\n",
    "            'tr24': [6, 1, 10], 'tr25': [2, 3, 12], 'tr26': [4, 1, 12], 'tr27': [2, 1, 14]\n",
    "        }\n",
    "\n",
    "        print(f\"ðŸš€ åˆå§‹åŒ– ColO-RAN Pro è™•ç†å™¨\")\n",
    "        print(f\"ðŸ“ è³‡æ–™é›†è·¯å¾‘: {self.dataset_path}\")\n",
    "        print(f\"ðŸŽ¯ ç›®æ¨™é…ç½®: å…¨éƒ¨ {len(self.training_configs)} å€‹é…ç½®\")\n",
    "        print(f\"ðŸ“Š é è¨ˆè¼‰å…¥çµ„åˆæ•¸: {len(self.scheduling_policies)} Ã— {len(self.training_configs)} Ã— {len(self.base_stations)} = {len(self.scheduling_policies) * len(self.training_configs) * len(self.base_stations)}\")\n",
    "\n",
    "    def auto_detect_structure(self):\n",
    "        \"\"\"è‡ªå‹•åµæ¸¬è³‡æ–™é›†çµæ§‹\"\"\"\n",
    "        print(\"ðŸ” è‡ªå‹•åµæ¸¬è³‡æ–™é›†çµæ§‹...\")\n",
    "\n",
    "        # å¯èƒ½çš„è·¯å¾‘çµæ§‹\n",
    "        possible_paths = [\n",
    "            f\"{self.dataset_path}/rome_static_medium\",\n",
    "            f\"{self.dataset_path}/colosseum-oran-coloran-dataset/rome_static_medium\",\n",
    "            self.dataset_path\n",
    "        ]\n",
    "\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                sched_dirs = [d for d in os.listdir(path) if d.startswith('sched')]\n",
    "                if sched_dirs:\n",
    "                    print(f\"âœ… æ‰¾åˆ°æœ‰æ•ˆçµæ§‹: {path}\")\n",
    "                    print(f\"ðŸ“‚ ç™¼ç¾æŽ’ç¨‹ç­–ç•¥: {sched_dirs}\")\n",
    "                    return path\n",
    "\n",
    "        print(\"âŒ æœªæ‰¾åˆ°æ¨™æº–è³‡æ–™çµæ§‹ï¼Œåˆ—å‡ºå¯ç”¨ç›®éŒ„ï¼š\")\n",
    "        if os.path.exists(self.dataset_path):\n",
    "            for item in os.listdir(self.dataset_path):\n",
    "                print(f\"  - {item}\")\n",
    "\n",
    "        return self.dataset_path\n",
    "\n",
    "    def load_all_data_with_glob_pro(self):\n",
    "        \"\"\"ä½¿ç”¨ glob è¼‰å…¥å…¨éƒ¨è³‡æ–™ï¼ˆPro ç‰ˆæœ¬ï¼Œç„¡è¨˜æ†¶é«”é™åˆ¶ï¼‰\"\"\"\n",
    "\n",
    "        # è‡ªå‹•åµæ¸¬è³‡æ–™çµæ§‹\n",
    "        base_data_path = self.auto_detect_structure()\n",
    "\n",
    "        bs_data_list = []\n",
    "        ue_data_list = []\n",
    "        slice_data_list = []\n",
    "\n",
    "        total_combinations = len(self.scheduling_policies) * len(self.training_configs) * len(self.base_stations)\n",
    "        current = 0\n",
    "        success_count = 0\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸš€ é–‹å§‹è¼‰å…¥å®Œæ•´ ColO-RAN è³‡æ–™é›†ï¼ˆå…¨éƒ¨ 588 å€‹çµ„åˆï¼‰\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        for sched_policy in self.scheduling_policies:\n",
    "            for training_config in self.training_configs:\n",
    "                print(f\"\\nðŸ“‹ è™•ç†é…ç½®: {sched_policy}/{training_config}\")\n",
    "\n",
    "                # å»ºæ§‹æœå°‹è·¯å¾‘\n",
    "                search_patterns = {\n",
    "                    'bs': f\"{base_data_path}/{sched_policy}/{training_config}/exp*/bs*/bs*.csv\",\n",
    "                    'ue': f\"{base_data_path}/{sched_policy}/{training_config}/exp*/bs*/ue*.csv\",\n",
    "                    'slice': f\"{base_data_path}/{sched_policy}/{training_config}/exp*/bs*/slices_bs*/*_metrics.csv\"\n",
    "                }\n",
    "\n",
    "                # è¼‰å…¥ BS è³‡æ–™\n",
    "                bs_files = glob.glob(search_patterns['bs'])\n",
    "                print(f\"  ðŸ“Š BS æª”æ¡ˆ: {len(bs_files)} å€‹\")\n",
    "\n",
    "                for bs_file in bs_files:\n",
    "                    current += 1\n",
    "                    try:\n",
    "                        df = pd.read_csv(bs_file)\n",
    "\n",
    "                        # è§£æžè·¯å¾‘è³‡è¨Š\n",
    "                        path_parts = bs_file.split('/')\n",
    "                        exp_folder = next((p for p in path_parts if p.startswith('exp')), 'exp1')\n",
    "                        bs_folder = next((p for p in path_parts if p.startswith('bs') and not p.endswith('.csv')), 'bs1')\n",
    "\n",
    "                        bs_id = int(bs_folder.replace('bs', ''))\n",
    "                        exp_id = int(exp_folder.replace('exp', ''))\n",
    "\n",
    "                        # æ·»åŠ  metadata\n",
    "                        df['bs_id'] = bs_id\n",
    "                        df['exp_id'] = exp_id\n",
    "                        df['sched_policy'] = sched_policy\n",
    "                        df['training_config'] = training_config\n",
    "                        df['file_path'] = bs_file\n",
    "\n",
    "                        bs_data_list.append(df)\n",
    "                        success_count += 1\n",
    "\n",
    "                        if current % 50 == 0:\n",
    "                            print(f\"    â³ é€²åº¦: {current}/{total_combinations} ({current/total_combinations*100:.1f}%)\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"    âŒ BS æª”æ¡ˆè¼‰å…¥å¤±æ•— {bs_file}: {e}\")\n",
    "\n",
    "                # è¼‰å…¥ UE è³‡æ–™\n",
    "                ue_files = glob.glob(search_patterns['ue'])\n",
    "                print(f\"  ðŸ“± UE æª”æ¡ˆ: {len(ue_files)} å€‹\")\n",
    "\n",
    "                for ue_file in ue_files:\n",
    "                    try:\n",
    "                        df = pd.read_csv(ue_file)\n",
    "\n",
    "                        # è§£æžè·¯å¾‘è³‡è¨Š\n",
    "                        path_parts = ue_file.split('/')\n",
    "                        exp_folder = next((p for p in path_parts if p.startswith('exp')), 'exp1')\n",
    "                        bs_folder = next((p for p in path_parts if p.startswith('bs') and not p.endswith('.csv')), 'bs1')\n",
    "                        ue_file_name = os.path.basename(ue_file)\n",
    "\n",
    "                        bs_id = int(bs_folder.replace('bs', ''))\n",
    "                        exp_id = int(exp_folder.replace('exp', ''))\n",
    "                        ue_id = int(ue_file_name.replace('ue', '').replace('.csv', ''))\n",
    "\n",
    "                        # æ·»åŠ  metadata\n",
    "                        df['bs_id'] = bs_id\n",
    "                        df['exp_id'] = exp_id\n",
    "                        df['ue_id'] = ue_id\n",
    "                        df['sched_policy'] = sched_policy\n",
    "                        df['training_config'] = training_config\n",
    "                        df['file_path'] = ue_file\n",
    "\n",
    "                        ue_data_list.append(df)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"    âŒ UE æª”æ¡ˆè¼‰å…¥å¤±æ•— {ue_file}: {e}\")\n",
    "\n",
    "                # è¼‰å…¥ Slice è³‡æ–™\n",
    "                slice_files = glob.glob(search_patterns['slice'])\n",
    "                print(f\"  ðŸ° Slice æª”æ¡ˆ: {len(slice_files)} å€‹\")\n",
    "\n",
    "                for slice_file in slice_files:\n",
    "                    try:\n",
    "                        df = pd.read_csv(slice_file)\n",
    "\n",
    "                        # è§£æžè·¯å¾‘è³‡è¨Š\n",
    "                        path_parts = slice_file.split('/')\n",
    "                        exp_folder = next((p for p in path_parts if p.startswith('exp')), 'exp1')\n",
    "                        bs_folder = next((p for p in path_parts if p.startswith('bs') and 'slices' not in p), 'bs1')\n",
    "                        slice_file_name = os.path.basename(slice_file)\n",
    "\n",
    "                        bs_id = int(bs_folder.replace('bs', ''))\n",
    "                        exp_id = int(exp_folder.replace('exp', ''))\n",
    "                        imsi = slice_file_name.replace('_metrics.csv', '')\n",
    "\n",
    "                        # æ·»åŠ  metadata\n",
    "                        df['bs_id'] = bs_id\n",
    "                        df['exp_id'] = exp_id\n",
    "                        df['imsi'] = imsi\n",
    "                        df['sched_policy'] = sched_policy\n",
    "                        df['training_config'] = training_config\n",
    "                        df['file_path'] = slice_file\n",
    "\n",
    "                        slice_data_list.append(df)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"    âŒ Slice æª”æ¡ˆè¼‰å…¥å¤±æ•— {slice_file}: {e}\")\n",
    "\n",
    "        # åˆä½µæ‰€æœ‰è³‡æ–™\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸ”— åˆä½µè³‡æ–™ä¸­...\")\n",
    "\n",
    "        combined_bs_data = pd.concat(bs_data_list, ignore_index=True) if bs_data_list else None\n",
    "        combined_ue_data = pd.concat(ue_data_list, ignore_index=True) if ue_data_list else None\n",
    "        combined_slice_data = pd.concat(slice_data_list, ignore_index=True) if slice_data_list else None\n",
    "\n",
    "        # è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³\n",
    "        def get_memory_usage(df, name):\n",
    "            if df is not None:\n",
    "                memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "                return f\"{name}: {len(df):,} ç­†è¨˜éŒ„, {memory_mb:.1f} MB\"\n",
    "            return f\"{name}: 0 ç­†è¨˜éŒ„\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸ“Š è¼‰å…¥å®Œæˆçµ±è¨ˆ\")\n",
    "        print(\"=\"*80)\n",
    "        print(get_memory_usage(combined_bs_data, \"åŸºç«™è³‡æ–™\"))\n",
    "        print(get_memory_usage(combined_ue_data, \"UEè³‡æ–™\"))\n",
    "        print(get_memory_usage(combined_slice_data, \"åˆ‡ç‰‡è³‡æ–™\"))\n",
    "        print(f\"âœ… æˆåŠŸè¼‰å…¥æª”æ¡ˆæ•¸: {success_count}\")\n",
    "        print(f\"ðŸŽ¯ è¼‰å…¥æˆåŠŸçŽ‡: {success_count/total_combinations*100:.1f}%\")\n",
    "\n",
    "        return combined_bs_data, combined_ue_data, combined_slice_data\n",
    "\n",
    "# åŸ·è¡Œå®Œæ•´è¼‰å…¥\n",
    "processor_pro = ColoRANDataProcessorPro(DATASET_PATH)\n",
    "bs_data_full, ue_data_full, slice_data_full = processor_pro.load_all_data_with_glob_pro()\n",
    "\n",
    "# é¡¯ç¤ºè¼‰å…¥çµæžœæ‘˜è¦\n",
    "print(\"\\n\" + \"ðŸŽ‰\" + \"=\"*78 + \"ðŸŽ‰\")\n",
    "print(\"ColO-RAN å®Œæ•´è³‡æ–™é›†è¼‰å…¥å®Œæˆï¼\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if bs_data_full is not None:\n",
    "    print(f\"ðŸ“Š åŸºç«™è³‡æ–™: {len(bs_data_full):,} ç­†è¨˜éŒ„\")\n",
    "    print(f\"   æŽ’ç¨‹ç­–ç•¥åˆ†ä½ˆ: {bs_data_full['sched_policy'].value_counts().to_dict()}\")\n",
    "    print(f\"   è¨“ç·´é…ç½®æ•¸é‡: {bs_data_full['training_config'].nunique()}\")\n",
    "    print(f\"   åŸºç«™æ•¸é‡: {bs_data_full['bs_id'].nunique()}\")\n",
    "\n",
    "if ue_data_full is not None:\n",
    "    print(f\"ðŸ“± UEè³‡æ–™: {len(ue_data_full):,} ç­†è¨˜éŒ„\")\n",
    "    print(f\"   UEè¨­å‚™æ•¸é‡: {ue_data_full['ue_id'].nunique()}\")\n",
    "\n",
    "if slice_data_full is not None:\n",
    "    print(f\"ðŸ° åˆ‡ç‰‡è³‡æ–™: {len(slice_data_full):,} ç­†è¨˜éŒ„\")\n",
    "    print(f\"   IMSIæ•¸é‡: {slice_data_full['imsi'].nunique()}\")\n",
    "\n",
    "print(\"\\nðŸš€ è³‡æ–™é›†å·²æº–å‚™å®Œæˆï¼Œå¯é€²è¡Œå¾ŒçºŒåˆ†æžï¼\")\n",
    "\n",
    "# ===== åœ¨ Cell 2 æœ€å¾ŒåŠ å…¥ä»¥ä¸‹ç¨‹å¼ç¢¼ =====\n",
    "\n",
    "# ä¿å­˜åŽŸå§‹è¼‰å…¥çš„è³‡æ–™ç‚º parquet æª”æ¡ˆ\n",
    "def save_raw_data_to_parquet():\n",
    "    \"\"\"å°‡è¼‰å…¥çš„åŽŸå§‹è³‡æ–™ä¿å­˜ç‚º parquet æª”æ¡ˆ\"\"\"\n",
    "    print(\"\\n\" + \"ðŸ’¾\" + \"=\"*78 + \"ðŸ’¾\")\n",
    "    print(\"ä¿å­˜åŽŸå§‹è³‡æ–™ç‚º Parquet æª”æ¡ˆ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    saved_files = []\n",
    "\n",
    "    # ä¿å­˜åŸºç«™è³‡æ–™\n",
    "    if bs_data_full is not None and len(bs_data_full) > 0:\n",
    "        bs_filename = 'raw_bs_data.parquet'\n",
    "        bs_data_full.to_parquet(bs_filename, compression='snappy', index=False)\n",
    "        file_size = os.path.getsize(bs_filename) / 1024 / 1024\n",
    "        print(f\"âœ… åŸºç«™è³‡æ–™å·²ä¿å­˜: {bs_filename} ({file_size:.1f} MB)\")\n",
    "        saved_files.append(bs_filename)\n",
    "\n",
    "    # ä¿å­˜ UE è³‡æ–™\n",
    "    if ue_data_full is not None and len(ue_data_full) > 0:\n",
    "        ue_filename = 'raw_ue_data.parquet'\n",
    "        ue_data_full.to_parquet(ue_filename, compression='snappy', index=False)\n",
    "        file_size = os.path.getsize(ue_filename) / 1024 / 1024\n",
    "        print(f\"âœ… UEè³‡æ–™å·²ä¿å­˜: {ue_filename} ({file_size:.1f} MB)\")\n",
    "        saved_files.append(ue_filename)\n",
    "\n",
    "    # ä¿å­˜åˆ‡ç‰‡è³‡æ–™\n",
    "    if slice_data_full is not None and len(slice_data_full) > 0:\n",
    "        slice_filename = 'raw_slice_data.parquet'\n",
    "        slice_data_full.to_parquet(slice_filename, compression='snappy', index=False)\n",
    "        file_size = os.path.getsize(slice_filename) / 1024 / 1024\n",
    "        print(f\"âœ… åˆ‡ç‰‡è³‡æ–™å·²ä¿å­˜: {slice_filename} ({file_size:.1f} MB)\")\n",
    "        saved_files.append(slice_filename)\n",
    "\n",
    "    # ä¿å­˜åˆ‡ç‰‡é…ç½®è³‡è¨Š\n",
    "    import json\n",
    "    config_filename = 'slice_configs.json'\n",
    "    with open(config_filename, 'w') as f:\n",
    "        json.dump(processor_pro.slice_configs, f, indent=2)\n",
    "    print(f\"âœ… åˆ‡ç‰‡é…ç½®å·²ä¿å­˜: {config_filename}\")\n",
    "    saved_files.append(config_filename)\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ æ‰€æœ‰åŽŸå§‹è³‡æ–™å·²æˆåŠŸä¿å­˜ï¼\")\n",
    "    print(f\"ðŸ“ ä¿å­˜æª”æ¡ˆæ¸…å–®: {saved_files}\")\n",
    "    print(f\"ðŸ’¡ ä¸‹æ¬¡å¯ç›´æŽ¥å¾ž Cell 3 é–‹å§‹åŸ·è¡Œï¼Œè‡ªå‹•è¼‰å…¥é€™äº›æª”æ¡ˆ\")\n",
    "\n",
    "    return saved_files\n",
    "\n",
    "# åŸ·è¡Œä¿å­˜\n",
    "saved_files = save_raw_data_to_parquet()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vFDUl3XT9-7"
   },
   "source": [
    "### 3. è³‡æ–™å‰è™•ç†èˆ‡ç‰¹å¾µå·¥ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 135298,
     "status": "ok",
     "timestamp": 1751587609316,
     "user": {
      "displayName": "Alex Tsai",
      "userId": "08548353837065676083"
     },
     "user_tz": -480
    },
    "id": "GI-AdULf0DB3",
    "outputId": "34ff872c-a724-409c-e409-df91dcdc5cee"
   },
   "outputs": [],
   "source": [
    "# @title è¨˜æ†¶é«”å„ªåŒ–ç‰¹å¾µå·¥ç¨‹è™•ç†å™¨ï¼ˆå«è‡ªå‹•æª¢æŸ¥é»žè®€å–èˆ‡åˆ†æ‰¹è™•ç†ï¼‰\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_raw_data_if_exists():\n",
    "    \"\"\"æª¢æŸ¥ä¸¦è¼‰å…¥å·²ä¿å­˜çš„åŽŸå§‹è³‡æ–™\"\"\"\n",
    "    print(\"ðŸ” æª¢æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿å­˜çš„åŽŸå§‹è³‡æ–™...\")\n",
    "\n",
    "    # æª¢æŸ¥å¿…è¦æª”æ¡ˆæ˜¯å¦å­˜åœ¨\n",
    "    required_files = [\n",
    "        'raw_bs_data.parquet',\n",
    "        'raw_ue_data.parquet',\n",
    "        'raw_slice_data.parquet',\n",
    "        'slice_configs.json'\n",
    "    ]\n",
    "\n",
    "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "\n",
    "    if missing_files:\n",
    "        print(f\"âŒ ç¼ºå°‘æª”æ¡ˆ: {missing_files}\")\n",
    "        print(\"ðŸ’¡ è«‹å…ˆåŸ·è¡Œ Cell 2 è¼‰å…¥åŽŸå§‹è³‡æ–™\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(\"âœ… æ‰¾åˆ°æ‰€æœ‰å¿…è¦æª”æ¡ˆï¼Œé–‹å§‹è¼‰å…¥...\")\n",
    "\n",
    "    try:\n",
    "        # è¼‰å…¥è³‡æ–™\n",
    "        bs_data = pd.read_parquet('raw_bs_data.parquet')\n",
    "        ue_data = pd.read_parquet('raw_ue_data.parquet')\n",
    "        slice_data = pd.read_parquet('raw_slice_data.parquet')\n",
    "\n",
    "        # è¼‰å…¥é…ç½®\n",
    "        with open('slice_configs.json', 'r') as f:\n",
    "            slice_configs = json.load(f)\n",
    "\n",
    "        print(f\"âœ… è³‡æ–™è¼‰å…¥å®Œæˆï¼\")\n",
    "        print(f\"   ðŸ“Š åŸºç«™è³‡æ–™: {len(bs_data):,} ç­†è¨˜éŒ„\")\n",
    "        print(f\"   ðŸ“± UEè³‡æ–™: {len(ue_data):,} ç­†è¨˜éŒ„\")\n",
    "        print(f\"   ðŸ° åˆ‡ç‰‡è³‡æ–™: {len(slice_data):,} ç­†è¨˜éŒ„\")\n",
    "        print(f\"   âš™ï¸ åˆ‡ç‰‡é…ç½®: {len(slice_configs)} å€‹\")\n",
    "\n",
    "        return bs_data, ue_data, slice_data, slice_configs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¼‰å…¥è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "class MemoryOptimizedNetworkSliceProcessor:\n",
    "    def __init__(self, slice_configs, batch_size=100000):\n",
    "        self.slice_configs = slice_configs\n",
    "        self.batch_size = batch_size\n",
    "        self.processed_batches = []\n",
    "\n",
    "        print(f\"ðŸ”§ åˆå§‹åŒ–è¨˜æ†¶é«”å„ªåŒ–è™•ç†å™¨\")\n",
    "        print(f\"ðŸ“¦ æ‰¹æ¬¡å¤§å°: {self.batch_size:,} ç­†è¨˜éŒ„\")\n",
    "\n",
    "    def process_data_in_batches(self, slice_data):\n",
    "        \"\"\"åˆ†æ‰¹è™•ç†å¤§åž‹è³‡æ–™é›†\"\"\"\n",
    "        print(f\"ðŸš€ é–‹å§‹åˆ†æ‰¹è™•ç†ï¼Œç¸½è¨˜éŒ„æ•¸: {len(slice_data):,}\")\n",
    "\n",
    "        total_rows = len(slice_data)\n",
    "        num_batches = (total_rows + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        processed_results = []\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * self.batch_size\n",
    "            end_idx = min((batch_idx + 1) * self.batch_size, total_rows)\n",
    "\n",
    "            print(f\"  ðŸ“¦ è™•ç†æ‰¹æ¬¡ {batch_idx + 1}/{num_batches} ({start_idx:,}-{end_idx:,})\")\n",
    "\n",
    "            # å–å¾—ç•¶å‰æ‰¹æ¬¡è³‡æ–™\n",
    "            batch_data = slice_data.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "            # è™•ç†ç•¶å‰æ‰¹æ¬¡\n",
    "            processed_batch = self._process_single_batch(batch_data)\n",
    "\n",
    "            if processed_batch is not None:\n",
    "                processed_results.append(processed_batch)\n",
    "                print(f\"    âœ… æ‰¹æ¬¡è™•ç†å®Œæˆ: {len(processed_batch):,} ç­†è¨˜éŒ„\")\n",
    "\n",
    "            # ç«‹å³é‡‹æ”¾è¨˜æ†¶é«”\n",
    "            del batch_data, processed_batch\n",
    "            gc.collect()\n",
    "\n",
    "            # é¡¯ç¤ºè¨˜æ†¶é«”ä½¿ç”¨ç‹€æ³\n",
    "            if (batch_idx + 1) % 5 == 0:\n",
    "                memory_info = self._get_memory_info()\n",
    "                print(f\"    ðŸ’¾ è¨˜æ†¶é«”ä½¿ç”¨: {memory_info}\")\n",
    "\n",
    "        # åˆä½µæ‰€æœ‰æ‰¹æ¬¡çµæžœ\n",
    "        if processed_results:\n",
    "            print(\"ðŸ”— åˆä½µæ‰€æœ‰æ‰¹æ¬¡çµæžœ...\")\n",
    "            final_result = pd.concat(processed_results, ignore_index=True)\n",
    "\n",
    "            # æ¸…ç†ä¸­é–“çµæžœ\n",
    "            del processed_results\n",
    "            gc.collect()\n",
    "\n",
    "            return final_result\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _process_single_batch(self, df):\n",
    "        \"\"\"è™•ç†å–®ä¸€æ‰¹æ¬¡çš„è³‡æ–™\"\"\"\n",
    "        try:\n",
    "            # 1. æ™‚é–“ç‰¹å¾µï¼ˆå‘é‡åŒ–ï¼‰\n",
    "            if 'Timestamp' in df.columns:\n",
    "                timestamps = pd.to_datetime(df['Timestamp'], unit='ms', errors='coerce')\n",
    "                df['hour'] = timestamps.dt.hour\n",
    "                df['minute'] = timestamps.dt.minute\n",
    "                df['day_of_week'] = timestamps.dt.dayofweek\n",
    "                # ä¸ä¿ç•™å®Œæ•´çš„ timestamp æ¬„ä½ä»¥ç¯€çœè¨˜æ†¶é«”\n",
    "                del timestamps\n",
    "\n",
    "            # 2. æŽ’ç¨‹ç­–ç•¥ç·¨ç¢¼ï¼ˆå‘é‡åŒ–ï¼‰\n",
    "            sched_mapping = {'sched0': 0, 'sched1': 1, 'sched2': 2}\n",
    "            df['sched_policy_num'] = df['sched_policy'].map(sched_mapping)\n",
    "\n",
    "            # 3. RBG é…ç½®ï¼ˆå‘é‡åŒ–å„ªåŒ–ï¼‰\n",
    "            df['allocated_rbgs'] = self._vectorized_rbg_allocation(df)\n",
    "\n",
    "            # 4. è³‡æºåˆ©ç”¨çŽ‡ï¼ˆå‘é‡åŒ–ï¼‰\n",
    "            df['sum_requested_prbs'] = df.get('sum_requested_prbs', 0).fillna(0)\n",
    "            df['sum_granted_prbs'] = df.get('sum_granted_prbs', 0).fillna(0)\n",
    "            df['prb_utilization'] = np.where(\n",
    "                df['sum_requested_prbs'] > 0,\n",
    "                df['sum_granted_prbs'] / df['sum_requested_prbs'],\n",
    "                0\n",
    "            ).clip(0, 1)\n",
    "\n",
    "            # 5. åžåé‡æ•ˆçŽ‡ï¼ˆå‘é‡åŒ–ï¼‰\n",
    "            throughput_col = 'tx_brate downlink [Mbps]'\n",
    "            if throughput_col in df.columns:\n",
    "                df['throughput_efficiency'] = np.where(\n",
    "                    df['sum_granted_prbs'] > 0,\n",
    "                    df[throughput_col].fillna(0) / df['sum_granted_prbs'],\n",
    "                    0\n",
    "                )\n",
    "            else:\n",
    "                df['throughput_efficiency'] = 0.0\n",
    "\n",
    "            # 6. QoS è©•åˆ†ï¼ˆå‘é‡åŒ–ï¼‰\n",
    "            df['qos_score'] = self._calculate_qos_score_vectorized(df)\n",
    "\n",
    "            # 7. ç¶²è·¯è² è¼‰\n",
    "            df['num_ues'] = df.get('num_ues', 1).fillna(1)\n",
    "            df['network_load'] = df['num_ues'] / 42.0\n",
    "\n",
    "            # 8. ç¶œåˆæ•ˆçŽ‡æŒ‡æ¨™\n",
    "            df['allocation_efficiency'] = (\n",
    "                0.5 * df['throughput_efficiency'] +\n",
    "                0.3 * df['qos_score'] +\n",
    "                0.2 * df['prb_utilization']\n",
    "            ).clip(0, 1)\n",
    "\n",
    "            # 9. é¸æ“‡éœ€è¦çš„æ¬„ä½ï¼Œç§»é™¤ä¸å¿…è¦çš„æ¬„ä½\n",
    "            required_columns = [\n",
    "                'num_ues', 'slice_id', 'sched_policy_num', 'allocated_rbgs',\n",
    "                'bs_id', 'exp_id', 'sum_requested_prbs', 'sum_granted_prbs',\n",
    "                'prb_utilization', 'throughput_efficiency', 'qos_score',\n",
    "                'network_load', 'hour', 'minute', 'day_of_week',\n",
    "                'allocation_efficiency', 'sched_policy', 'training_config'\n",
    "            ]\n",
    "\n",
    "            available_columns = [col for col in required_columns if col in df.columns]\n",
    "            df_result = df[available_columns].copy()\n",
    "\n",
    "            # æ¸…ç†ç•°å¸¸å€¼\n",
    "            df_result = df_result.dropna(subset=['allocation_efficiency'])\n",
    "\n",
    "            return df_result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    âŒ æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _vectorized_rbg_allocation(self, df):\n",
    "        \"\"\"å‘é‡åŒ–çš„ RBG é…ç½®è¨ˆç®—\"\"\"\n",
    "        # å‰µå»ºé…ç½®å°æ‡‰å­—å…¸\n",
    "        config_map = {}\n",
    "        for config_name, rbg_list in self.slice_configs.items():\n",
    "            for slice_id in range(len(rbg_list)):\n",
    "                config_map[(config_name, slice_id)] = rbg_list[slice_id]\n",
    "\n",
    "        # å‘é‡åŒ–æ˜ å°„\n",
    "        allocation_keys = list(zip(df['training_config'], df.get('slice_id', 0).fillna(0)))\n",
    "        return pd.Series(allocation_keys).map(config_map).fillna(0).values\n",
    "\n",
    "    def _calculate_qos_score_vectorized(self, df):\n",
    "        \"\"\"å‘é‡åŒ–çš„ QoS è©•åˆ†è¨ˆç®—\"\"\"\n",
    "        dl_error_col = 'tx_errors downlink (%)'\n",
    "        ul_error_col = 'rx_errors uplink (%)'\n",
    "        cqi_col = 'dl_cqi'\n",
    "\n",
    "        dl_score = 0.5  # é è¨­å€¼\n",
    "        ul_score = 0.5  # é è¨­å€¼\n",
    "        cqi_score = 0.5  # é è¨­å€¼\n",
    "\n",
    "        if dl_error_col in df.columns:\n",
    "            dl_score = (100 - df[dl_error_col].fillna(50)) / 100\n",
    "\n",
    "        if ul_error_col in df.columns:\n",
    "            ul_score = (100 - df[ul_error_col].fillna(50)) / 100\n",
    "\n",
    "        if cqi_col in df.columns:\n",
    "            cqi_score = df[cqi_col].fillna(7.5) / 15\n",
    "\n",
    "        return (0.4 * dl_score + 0.3 * ul_score + 0.3 * cqi_score).clip(0, 1)\n",
    "\n",
    "    def _get_memory_info(self):\n",
    "        \"\"\"å–å¾—è¨˜æ†¶é«”ä½¿ç”¨è³‡è¨Š\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process()\n",
    "            memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "            return f\"{memory_mb:.1f} MB\"\n",
    "        except:\n",
    "            return \"ç„¡æ³•å–å¾—\"\n",
    "\n",
    "def optimize_datatypes(df):\n",
    "    \"\"\"æœ€ä½³åŒ–è³‡æ–™åž‹åˆ¥ä»¥ç¯€çœè¨˜æ†¶é«”\"\"\"\n",
    "    print(\"ðŸ”§ æœ€ä½³åŒ–è³‡æ–™åž‹åˆ¥...\")\n",
    "\n",
    "    initial_memory = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "\n",
    "    # æ•´æ•¸åž‹åˆ¥æœ€ä½³åŒ–\n",
    "    int_cols = df.select_dtypes(include=['int64']).columns\n",
    "    for col in int_cols:\n",
    "        col_min, col_max = df[col].min(), df[col].max()\n",
    "        if col_min >= 0 and col_max < 255:\n",
    "            df[col] = df[col].astype('uint8')\n",
    "        elif col_min >= 0 and col_max < 65535:\n",
    "            df[col] = df[col].astype('uint16')\n",
    "        elif col_min >= -128 and col_max < 127:\n",
    "            df[col] = df[col].astype('int8')\n",
    "        elif col_min >= -32768 and col_max < 32767:\n",
    "            df[col] = df[col].astype('int16')\n",
    "\n",
    "    # æµ®é»žæ•¸åž‹åˆ¥æœ€ä½³åŒ–\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    for col in float_cols:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "    # é¡žåˆ¥åž‹è³‡æ–™\n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in object_cols:\n",
    "        if df[col].nunique() / len(df) < 0.5:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    final_memory = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    print(f\"  ðŸ’¾ è¨˜æ†¶é«”æœ€ä½³åŒ–: {initial_memory:.1f} MB â†’ {final_memory:.1f} MB\")\n",
    "    print(f\"  ðŸ“‰ ç¯€çœ: {((initial_memory - final_memory) / initial_memory * 100):.1f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_processed_data_to_parquet(processed_data, feature_names):\n",
    "    \"\"\"ä¿å­˜è™•ç†å¾Œçš„è³‡æ–™ç‚º Parquet æ ¼å¼\"\"\"\n",
    "    if processed_data is None or len(processed_data) == 0:\n",
    "        print(\"âŒ æ²’æœ‰è³‡æ–™å¯å„²å­˜\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nðŸ’¾ ä¿å­˜è™•ç†å¾Œçš„è³‡æ–™...\")\n",
    "\n",
    "    # æœ€ä½³åŒ–è³‡æ–™åž‹åˆ¥\n",
    "    optimized_data = optimize_datatypes(processed_data.copy())\n",
    "\n",
    "    # ä¿å­˜ç‚º Parquet\n",
    "    output_filename = 'coloran_processed_features.parquet'\n",
    "    optimized_data.to_parquet(\n",
    "        output_filename,\n",
    "        compression='snappy',\n",
    "        index=False,\n",
    "        engine='pyarrow'\n",
    "    )\n",
    "\n",
    "    # è¨ˆç®—æª”æ¡ˆå¤§å°\n",
    "    file_size = os.path.getsize(output_filename) / 1024 / 1024\n",
    "    memory_size = optimized_data.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "\n",
    "    print(f\"âœ… ç‰¹å¾µè³‡æ–™å·²ä¿å­˜: {output_filename}\")\n",
    "    print(f\"ðŸ“Š è¨˜éŒ„æ•¸é‡: {len(optimized_data):,}\")\n",
    "    print(f\"ðŸ“‹ ç‰¹å¾µæ•¸é‡: {len(feature_names)}\")\n",
    "    print(f\"ðŸ’¾ æª”æ¡ˆå¤§å°: {file_size:.1f} MB\")\n",
    "    print(f\"ðŸ—œï¸ å£“ç¸®æ•ˆçŽ‡: {memory_size/file_size:.1f}x\")\n",
    "\n",
    "    # ä¿å­˜ç‰¹å¾µåç¨±å’Œå…ƒè³‡æ–™\n",
    "    feature_info = {\n",
    "        'feature_names': feature_names,\n",
    "        'total_records': len(optimized_data),\n",
    "        'processing_date': datetime.now().isoformat(),\n",
    "        'file_size_mb': file_size,\n",
    "        'compression_ratio': memory_size/file_size\n",
    "    }\n",
    "\n",
    "    with open('feature_metadata.json', 'w') as f:\n",
    "        json.dump(feature_info, f, indent=2)\n",
    "\n",
    "    print(f\"ðŸ“‹ ç‰¹å¾µå…ƒè³‡æ–™å·²ä¿å­˜: feature_metadata.json\")\n",
    "\n",
    "    # æ¸…ç†è¨˜æ†¶é«”\n",
    "    del optimized_data\n",
    "    gc.collect()\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "def main_processing_pipeline():\n",
    "    \"\"\"ä¸»è¦è™•ç†ç®¡é“\"\"\"\n",
    "    print(\"ðŸš€ å•Ÿå‹•è¨˜æ†¶é«”å„ªåŒ–ç‰¹å¾µå·¥ç¨‹ç®¡é“\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. è¼‰å…¥åŽŸå§‹è³‡æ–™\n",
    "    bs_data_loaded, ue_data_loaded, slice_data_loaded, slice_configs_loaded = load_raw_data_if_exists()\n",
    "\n",
    "    # å¦‚æžœç„¡æ³•è¼‰å…¥ä¿å­˜çš„æª”æ¡ˆï¼Œå‰‡å˜—è©¦ä½¿ç”¨ Cell 2 çš„è®Šæ•¸\n",
    "    if slice_data_loaded is None:\n",
    "        print(\"âš ï¸ å˜—è©¦ä½¿ç”¨ Cell 2 çš„è¨˜æ†¶é«”è®Šæ•¸...\")\n",
    "        if 'bs_data_full' in globals():\n",
    "            bs_data_loaded = bs_data_full\n",
    "            ue_data_loaded = ue_data_full\n",
    "            slice_data_loaded = slice_data_full\n",
    "            slice_configs_loaded = processor_pro.slice_configs\n",
    "            print(\"âœ… æˆåŠŸä½¿ç”¨ Cell 2 çš„è¨˜æ†¶é«”è³‡æ–™\")\n",
    "        else:\n",
    "            print(\"âŒ è«‹å…ˆåŸ·è¡Œ Cell 2 è¼‰å…¥è³‡æ–™\")\n",
    "            return None, None\n",
    "\n",
    "    if slice_data_loaded is None or len(slice_data_loaded) == 0:\n",
    "        print(\"âŒ ç„¡å¯ç”¨çš„åˆ‡ç‰‡è³‡æ–™\")\n",
    "        return None, None\n",
    "\n",
    "    # 2. åˆå§‹åŒ–è¨˜æ†¶é«”å„ªåŒ–è™•ç†å™¨\n",
    "    print(f\"\\nðŸ”§ åˆå§‹åŒ–è™•ç†å™¨...\")\n",
    "    processor = MemoryOptimizedNetworkSliceProcessor(\n",
    "        slice_configs_loaded,\n",
    "        batch_size=75000  # æ ¹æ“š 83.5 GB RAM èª¿æ•´çš„æ‰¹æ¬¡å¤§å°\n",
    "    )\n",
    "\n",
    "    # 3. åˆ†æ‰¹è™•ç†è³‡æ–™\n",
    "    print(f\"\\nðŸ“Š é–‹å§‹åˆ†æ‰¹ç‰¹å¾µå·¥ç¨‹...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    processed_data = processor.process_data_in_batches(slice_data_loaded)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "    if processed_data is None:\n",
    "        print(\"âŒ ç‰¹å¾µå·¥ç¨‹è™•ç†å¤±æ•—\")\n",
    "        return None, None\n",
    "\n",
    "    # 4. ç«‹å³æ¸…ç†åŽŸå§‹è³‡æ–™ä»¥é‡‹æ”¾è¨˜æ†¶é«”\n",
    "    del bs_data_loaded, ue_data_loaded, slice_data_loaded\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆ!\")\n",
    "    print(f\"â±ï¸ è™•ç†æ™‚é–“: {processing_time:.2f} ç§’\")\n",
    "    print(f\"ðŸ“Š æœ€çµ‚è¨˜éŒ„æ•¸: {len(processed_data):,}\")\n",
    "\n",
    "    # 5. å»ºç«‹ç‰¹å¾µåˆ—è¡¨\n",
    "    feature_columns = [\n",
    "        'num_ues', 'slice_id', 'sched_policy_num', 'allocated_rbgs',\n",
    "        'bs_id', 'exp_id', 'sum_requested_prbs', 'sum_granted_prbs',\n",
    "        'prb_utilization', 'throughput_efficiency', 'qos_score',\n",
    "        'network_load', 'hour', 'minute', 'day_of_week'\n",
    "    ]\n",
    "\n",
    "    available_features = [f for f in feature_columns if f in processed_data.columns]\n",
    "\n",
    "    print(f\"\\nðŸ“‹ å¯ç”¨ç‰¹å¾µ ({len(available_features)} å€‹):\")\n",
    "    for i, feature in enumerate(available_features, 1):\n",
    "        print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "    # 6. ä¿å­˜è™•ç†å¾Œçš„è³‡æ–™\n",
    "    saved_file = save_processed_data_to_parquet(processed_data, available_features)\n",
    "\n",
    "    # 7. é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š\n",
    "    if 'allocation_efficiency' in processed_data.columns:\n",
    "        print(f\"\\nðŸ“ˆ ç›®æ¨™è®Šæ•¸çµ±è¨ˆ:\")\n",
    "        stats = processed_data['allocation_efficiency'].describe()\n",
    "        print(f\"   å¹³å‡: {stats['mean']:.4f}\")\n",
    "        print(f\"   æ¨™æº–å·®: {stats['std']:.4f}\")\n",
    "        print(f\"   ç¯„åœ: {stats['min']:.4f} - {stats['max']:.4f}\")\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ Cell 3 åŸ·è¡Œå®Œæˆï¼\")\n",
    "    print(f\"ðŸ“ è¼¸å‡ºæª”æ¡ˆ: {saved_file}\")\n",
    "    print(f\"ðŸ’¡ ä¸‹ä¸€æ­¥: åŸ·è¡Œ Cell 4 é€²è¡Œæ©Ÿå™¨å­¸ç¿’è¨“ç·´\")\n",
    "\n",
    "    return processed_data, available_features\n",
    "\n",
    "# åŸ·è¡Œä¸»è¦è™•ç†ç®¡é“\n",
    "processed_data_full, feature_names_full = main_processing_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPdmCUyZS2ErJIGfuOhpDYg",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
