{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ä½¿ç”¨ TensorFlow GPU åŠ é€Ÿå–ä»£ RAPIDS\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# æª¢æŸ¥ GPU å¯ç”¨æ€§\n",
        "print(f\"GPU å¯ç”¨: {tf.config.list_physical_devices('GPU')}\")\n",
        "\n",
        "class TensorFlowGPUProcessor:\n",
        "    \"\"\"ä½¿ç”¨ TensorFlow GPU åŠ é€Ÿçš„è³‡æ–™è™•ç†å™¨\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size=300000):\n",
        "        self.batch_size = batch_size\n",
        "        # è¨­å®š GPU è¨˜æ†¶é«”å¢žé•·\n",
        "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "    def process_with_gpu_acceleration(self, data):\n",
        "        \"\"\"ä½¿ç”¨ TensorFlow GPU é€²è¡Œå‘é‡åŒ–è™•ç†\"\"\"\n",
        "        with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
        "            # è½‰æ›ç‚º TensorFlow tensors\n",
        "            numeric_data = tf.constant(data.select_dtypes(include=[np.number]).values, dtype=tf.float32)\n",
        "\n",
        "            # GPU å‘é‡åŒ–é‹ç®—\n",
        "            processed_data = tf.nn.relu(numeric_data)  # ç¤ºä¾‹é‹ç®—\n",
        "\n",
        "            # è½‰å›ž pandas\n",
        "            return pd.DataFrame(processed_data.numpy(), columns=data.select_dtypes(include=[np.number]).columns)\n",
        "\n",
        "# ä½¿ç”¨æ–¹æ³•\n",
        "processor = TensorFlowGPUProcessor()\n"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXYHYVN_aPZX",
        "outputId": "f8deae92-38a0-4a9f-e276-d53b782a44ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU å¯ç”¨: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovA-JyqbUHce"
      },
      "source": [
        "### 1. ç’°å¢ƒè¨­ç½®èˆ‡è³‡æ–™è¼‰å…¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0H-_4xOoTP_O"
      },
      "outputs": [],
      "source": [
        "# @title ä¸‹è¼‰ ColO-RAN è³‡æ–™é›†åˆ° Colab æª”æ¡ˆç©ºé–“\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# æª¢æŸ¥ä¸¦ä¸‹è¼‰è³‡æ–™é›†\n",
        "dataset_repo_url = \"https://github.com/wineslab/colosseum-oran-coloran-dataset.git\"\n",
        "dataset_local_path = \"/content/colosseum-oran-coloran-dataset\"\n",
        "\n",
        "print(\"é–‹å§‹ä¸‹è¼‰ ColO-RAN è³‡æ–™é›†...\")\n",
        "print(f\"ä¸‹è¼‰ä½ç½®: {dataset_local_path}\")\n",
        "\n",
        "# å¦‚æžœè³‡æ–™å¤¾å·²å­˜åœ¨ï¼Œå…ˆåˆªé™¤\n",
        "if os.path.exists(dataset_local_path):\n",
        "    print(\"ç™¼ç¾ç¾æœ‰è³‡æ–™å¤¾ï¼Œæ­£åœ¨æ¸…ç†...\")\n",
        "    !rm -rf {dataset_local_path}\n",
        "\n",
        "# Git clone è³‡æ–™é›†\n",
        "try:\n",
        "    result = subprocess.run([\n",
        "        \"git\", \"clone\", dataset_repo_url, dataset_local_path\n",
        "    ], capture_output=True, text=True, timeout=600)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(\"âœ… è³‡æ–™é›†ä¸‹è¼‰æˆåŠŸï¼\")\n",
        "\n",
        "        # æª¢æŸ¥ä¸‹è¼‰çš„å…§å®¹\n",
        "        print(\"\\nðŸ“ è³‡æ–™é›†çµæ§‹é è¦½ï¼š\")\n",
        "        !ls -la {dataset_local_path}\n",
        "\n",
        "        # æª¢æŸ¥ rome_static_medium è³‡æ–™å¤¾\n",
        "        rome_path = f\"{dataset_local_path}/rome_static_medium\"\n",
        "        if os.path.exists(rome_path):\n",
        "            print(f\"\\nâœ… æ‰¾åˆ° rome_static_medium è³‡æ–™å¤¾\")\n",
        "            !ls -la {rome_path}\n",
        "        else:\n",
        "            print(f\"\\nâŒ æœªæ‰¾åˆ° rome_static_medium è³‡æ–™å¤¾ï¼Œåˆ—å‡ºæ‰€æœ‰å…§å®¹ï¼š\")\n",
        "            !find {dataset_local_path} -type d -maxdepth 2\n",
        "\n",
        "    else:\n",
        "        print(f\"âŒ ä¸‹è¼‰å¤±æ•—: {result.stderr}\")\n",
        "\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"âŒ ä¸‹è¼‰è¶…æ™‚ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·š\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ä¸‹è¼‰éŽç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "\n",
        "# è¨­å®šæ–°çš„è³‡æ–™é›†è·¯å¾‘\n",
        "DATASET_PATH = dataset_local_path\n",
        "print(f\"\\nðŸŽ¯ è³‡æ–™é›†è·¯å¾‘å·²è¨­å®šç‚º: {DATASET_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiiPPUYQT8sx"
      },
      "source": [
        "### 2. è³‡æ–™é›†è¼‰å…¥èˆ‡æ•´åˆå‡½æ•¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "xzQaeDGnT2fv"
      },
      "outputs": [],
      "source": [
        "# @title å®Œæ•´ç‰ˆ ColO-RAN è³‡æ–™è™•ç†å™¨ï¼ˆè¼‰å…¥å…¨éƒ¨ 588 å€‹çµ„åˆï¼‰\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ColoRANDataProcessorPro:\n",
        "    def __init__(self, dataset_path):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.base_stations = [1, 8, 15, 22, 29, 36, 43]\n",
        "        self.scheduling_policies = ['sched0', 'sched1', 'sched2']  # RR, WF, PF\n",
        "\n",
        "        # å…¨éƒ¨ 28 å€‹è¨“ç·´é…ç½® - ä¸å†é™åˆ¶è¨˜æ†¶é«”\n",
        "        self.training_configs = [f'tr{i}' for i in range(28)]\n",
        "\n",
        "        # åˆ‡ç‰‡é…ç½®å®šç¾©ï¼ˆå®Œæ•´ç‰ˆï¼‰\n",
        "        self.slice_configs = {\n",
        "            'tr0': [2, 13, 2], 'tr1': [4, 11, 2], 'tr2': [6, 9, 2], 'tr3': [8, 7, 2],\n",
        "            'tr4': [10, 5, 2], 'tr5': [12, 3, 2], 'tr6': [14, 1, 2], 'tr7': [2, 11, 4],\n",
        "            'tr8': [4, 9, 4], 'tr9': [6, 7, 4], 'tr10': [8, 5, 4], 'tr11': [10, 3, 4],\n",
        "            'tr12': [12, 1, 4], 'tr13': [2, 9, 6], 'tr14': [4, 7, 6], 'tr15': [6, 5, 6],\n",
        "            'tr16': [8, 3, 6], 'tr17': [10, 1, 6], 'tr18': [2, 7, 8], 'tr19': [4, 5, 8],\n",
        "            'tr20': [6, 3, 8], 'tr21': [8, 1, 8], 'tr22': [2, 5, 10], 'tr23': [4, 3, 10],\n",
        "            'tr24': [6, 1, 10], 'tr25': [2, 3, 12], 'tr26': [4, 1, 12], 'tr27': [2, 1, 14]\n",
        "        }\n",
        "\n",
        "        print(f\"ðŸš€ åˆå§‹åŒ– ColO-RAN Pro è™•ç†å™¨\")\n",
        "        print(f\"ðŸ“ è³‡æ–™é›†è·¯å¾‘: {self.dataset_path}\")\n",
        "        print(f\"ðŸŽ¯ ç›®æ¨™é…ç½®: å…¨éƒ¨ {len(self.training_configs)} å€‹é…ç½®\")\n",
        "        print(f\"ðŸ“Š é è¨ˆè¼‰å…¥çµ„åˆæ•¸: {len(self.scheduling_policies)} Ã— {len(self.training_configs)} Ã— {len(self.base_stations)} = {len(self.scheduling_policies) * len(self.training_configs) * len(self.base_stations)}\")\n",
        "\n",
        "    def auto_detect_structure(self):\n",
        "        \"\"\"è‡ªå‹•åµæ¸¬è³‡æ–™é›†çµæ§‹\"\"\"\n",
        "        print(\"ðŸ” è‡ªå‹•åµæ¸¬è³‡æ–™é›†çµæ§‹...\")\n",
        "\n",
        "        # å¯èƒ½çš„è·¯å¾‘çµæ§‹\n",
        "        possible_paths = [\n",
        "            f\"{self.dataset_path}/rome_static_medium\",\n",
        "            f\"{self.dataset_path}/colosseum-oran-coloran-dataset/rome_static_medium\",\n",
        "            self.dataset_path\n",
        "        ]\n",
        "\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                sched_dirs = [d for d in os.listdir(path) if d.startswith('sched')]\n",
        "                if sched_dirs:\n",
        "                    print(f\"âœ… æ‰¾åˆ°æœ‰æ•ˆçµæ§‹: {path}\")\n",
        "                    print(f\"ðŸ“‚ ç™¼ç¾æŽ’ç¨‹ç­–ç•¥: {sched_dirs}\")\n",
        "                    return path\n",
        "\n",
        "        print(\"âŒ æœªæ‰¾åˆ°æ¨™æº–è³‡æ–™çµæ§‹ï¼Œåˆ—å‡ºå¯ç”¨ç›®éŒ„ï¼š\")\n",
        "        if os.path.exists(self.dataset_path):\n",
        "            for item in os.listdir(self.dataset_path):\n",
        "                print(f\"  - {item}\")\n",
        "\n",
        "        return self.dataset_path\n",
        "\n",
        "    def load_all_data_with_glob_pro(self):\n",
        "        \"\"\"ä½¿ç”¨ glob è¼‰å…¥å…¨éƒ¨è³‡æ–™ï¼ˆPro ç‰ˆæœ¬ï¼Œç„¡è¨˜æ†¶é«”é™åˆ¶ï¼‰\"\"\"\n",
        "\n",
        "        # è‡ªå‹•åµæ¸¬è³‡æ–™çµæ§‹\n",
        "        base_data_path = self.auto_detect_structure()\n",
        "\n",
        "        bs_data_list = []\n",
        "        ue_data_list = []\n",
        "        slice_data_list = []\n",
        "\n",
        "        total_combinations = len(self.scheduling_policies) * len(self.training_configs) * len(self.base_stations)\n",
        "        current = 0\n",
        "        success_count = 0\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ðŸš€ é–‹å§‹è¼‰å…¥å®Œæ•´ ColO-RAN è³‡æ–™é›†ï¼ˆå…¨éƒ¨ 588 å€‹çµ„åˆï¼‰\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        for sched_policy in self.scheduling_policies:\n",
        "            for training_config in self.training_configs:\n",
        "                print(f\"\\nðŸ“‹ è™•ç†é…ç½®: {sched_policy}/{training_config}\")\n",
        "\n",
        "                # å»ºæ§‹æœå°‹è·¯å¾‘\n",
        "                search_patterns = {\n",
        "                    'bs': f\"{base_data_path}/{sched_policy}/{training_config}/exp*/bs*/bs*.csv\",\n",
        "                    'ue': f\"{base_data_path}/{sched_policy}/{training_config}/exp*/bs*/ue*.csv\",\n",
        "                    'slice': f\"{base_data_path}/{sched_policy}/{training_config}/exp*/bs*/slices_bs*/*_metrics.csv\"\n",
        "                }\n",
        "\n",
        "                # è¼‰å…¥ BS è³‡æ–™\n",
        "                bs_files = glob.glob(search_patterns['bs'])\n",
        "                print(f\"  ðŸ“Š BS æª”æ¡ˆ: {len(bs_files)} å€‹\")\n",
        "\n",
        "                for bs_file in bs_files:\n",
        "                    current += 1\n",
        "                    try:\n",
        "                        df = pd.read_csv(bs_file)\n",
        "\n",
        "                        # è§£æžè·¯å¾‘è³‡è¨Š\n",
        "                        path_parts = bs_file.split('/')\n",
        "                        exp_folder = next((p for p in path_parts if p.startswith('exp')), 'exp1')\n",
        "                        bs_folder = next((p for p in path_parts if p.startswith('bs') and not p.endswith('.csv')), 'bs1')\n",
        "\n",
        "                        bs_id = int(bs_folder.replace('bs', ''))\n",
        "                        exp_id = int(exp_folder.replace('exp', ''))\n",
        "\n",
        "                        # æ·»åŠ  metadata\n",
        "                        df['bs_id'] = bs_id\n",
        "                        df['exp_id'] = exp_id\n",
        "                        df['sched_policy'] = sched_policy\n",
        "                        df['training_config'] = training_config\n",
        "                        df['file_path'] = bs_file\n",
        "\n",
        "                        bs_data_list.append(df)\n",
        "                        success_count += 1\n",
        "\n",
        "                        if current % 50 == 0:\n",
        "                            print(f\"    â³ é€²åº¦: {current}/{total_combinations} ({current/total_combinations*100:.1f}%)\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"    âŒ BS æª”æ¡ˆè¼‰å…¥å¤±æ•— {bs_file}: {e}\")\n",
        "\n",
        "                # è¼‰å…¥ UE è³‡æ–™\n",
        "                ue_files = glob.glob(search_patterns['ue'])\n",
        "                print(f\"  ðŸ“± UE æª”æ¡ˆ: {len(ue_files)} å€‹\")\n",
        "\n",
        "                for ue_file in ue_files:\n",
        "                    try:\n",
        "                        df = pd.read_csv(ue_file)\n",
        "\n",
        "                        # è§£æžè·¯å¾‘è³‡è¨Š\n",
        "                        path_parts = ue_file.split('/')\n",
        "                        exp_folder = next((p for p in path_parts if p.startswith('exp')), 'exp1')\n",
        "                        bs_folder = next((p for p in path_parts if p.startswith('bs') and not p.endswith('.csv')), 'bs1')\n",
        "                        ue_file_name = os.path.basename(ue_file)\n",
        "\n",
        "                        bs_id = int(bs_folder.replace('bs', ''))\n",
        "                        exp_id = int(exp_folder.replace('exp', ''))\n",
        "                        ue_id = int(ue_file_name.replace('ue', '').replace('.csv', ''))\n",
        "\n",
        "                        # æ·»åŠ  metadata\n",
        "                        df['bs_id'] = bs_id\n",
        "                        df['exp_id'] = exp_id\n",
        "                        df['ue_id'] = ue_id\n",
        "                        df['sched_policy'] = sched_policy\n",
        "                        df['training_config'] = training_config\n",
        "                        df['file_path'] = ue_file\n",
        "\n",
        "                        ue_data_list.append(df)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"    âŒ UE æª”æ¡ˆè¼‰å…¥å¤±æ•— {ue_file}: {e}\")\n",
        "\n",
        "                # è¼‰å…¥ Slice è³‡æ–™\n",
        "                slice_files = glob.glob(search_patterns['slice'])\n",
        "                print(f\"  ðŸ° Slice æª”æ¡ˆ: {len(slice_files)} å€‹\")\n",
        "\n",
        "                for slice_file in slice_files:\n",
        "                    try:\n",
        "                        df = pd.read_csv(slice_file)\n",
        "\n",
        "                        # è§£æžè·¯å¾‘è³‡è¨Š\n",
        "                        path_parts = slice_file.split('/')\n",
        "                        exp_folder = next((p for p in path_parts if p.startswith('exp')), 'exp1')\n",
        "                        bs_folder = next((p for p in path_parts if p.startswith('bs') and 'slices' not in p), 'bs1')\n",
        "                        slice_file_name = os.path.basename(slice_file)\n",
        "\n",
        "                        bs_id = int(bs_folder.replace('bs', ''))\n",
        "                        exp_id = int(exp_folder.replace('exp', ''))\n",
        "                        imsi = slice_file_name.replace('_metrics.csv', '')\n",
        "\n",
        "                        # æ·»åŠ  metadata\n",
        "                        df['bs_id'] = bs_id\n",
        "                        df['exp_id'] = exp_id\n",
        "                        df['imsi'] = imsi\n",
        "                        df['sched_policy'] = sched_policy\n",
        "                        df['training_config'] = training_config\n",
        "                        df['file_path'] = slice_file\n",
        "\n",
        "                        slice_data_list.append(df)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"    âŒ Slice æª”æ¡ˆè¼‰å…¥å¤±æ•— {slice_file}: {e}\")\n",
        "\n",
        "        # åˆä½µæ‰€æœ‰è³‡æ–™\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ðŸ”— åˆä½µè³‡æ–™ä¸­...\")\n",
        "\n",
        "        combined_bs_data = pd.concat(bs_data_list, ignore_index=True) if bs_data_list else None\n",
        "        combined_ue_data = pd.concat(ue_data_list, ignore_index=True) if ue_data_list else None\n",
        "        combined_slice_data = pd.concat(slice_data_list, ignore_index=True) if slice_data_list else None\n",
        "\n",
        "        # è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³\n",
        "        def get_memory_usage(df, name):\n",
        "            if df is not None:\n",
        "                memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
        "                return f\"{name}: {len(df):,} ç­†è¨˜éŒ„, {memory_mb:.1f} MB\"\n",
        "            return f\"{name}: 0 ç­†è¨˜éŒ„\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ðŸ“Š è¼‰å…¥å®Œæˆçµ±è¨ˆ\")\n",
        "        print(\"=\"*80)\n",
        "        print(get_memory_usage(combined_bs_data, \"åŸºç«™è³‡æ–™\"))\n",
        "        print(get_memory_usage(combined_ue_data, \"UEè³‡æ–™\"))\n",
        "        print(get_memory_usage(combined_slice_data, \"åˆ‡ç‰‡è³‡æ–™\"))\n",
        "        print(f\"âœ… æˆåŠŸè¼‰å…¥æª”æ¡ˆæ•¸: {success_count}\")\n",
        "        print(f\"ðŸŽ¯ è¼‰å…¥æˆåŠŸçŽ‡: {success_count/total_combinations*100:.1f}%\")\n",
        "\n",
        "        return combined_bs_data, combined_ue_data, combined_slice_data\n",
        "\n",
        "# åŸ·è¡Œå®Œæ•´è¼‰å…¥\n",
        "processor_pro = ColoRANDataProcessorPro(DATASET_PATH)\n",
        "bs_data_full, ue_data_full, slice_data_full = processor_pro.load_all_data_with_glob_pro()\n",
        "\n",
        "# é¡¯ç¤ºè¼‰å…¥çµæžœæ‘˜è¦\n",
        "print(\"\\n\" + \"ðŸŽ‰\" + \"=\"*78 + \"ðŸŽ‰\")\n",
        "print(\"ColO-RAN å®Œæ•´è³‡æ–™é›†è¼‰å…¥å®Œæˆï¼\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if bs_data_full is not None:\n",
        "    print(f\"ðŸ“Š åŸºç«™è³‡æ–™: {len(bs_data_full):,} ç­†è¨˜éŒ„\")\n",
        "    print(f\"   æŽ’ç¨‹ç­–ç•¥åˆ†ä½ˆ: {bs_data_full['sched_policy'].value_counts().to_dict()}\")\n",
        "    print(f\"   è¨“ç·´é…ç½®æ•¸é‡: {bs_data_full['training_config'].nunique()}\")\n",
        "    print(f\"   åŸºç«™æ•¸é‡: {bs_data_full['bs_id'].nunique()}\")\n",
        "\n",
        "if ue_data_full is not None:\n",
        "    print(f\"ðŸ“± UEè³‡æ–™: {len(ue_data_full):,} ç­†è¨˜éŒ„\")\n",
        "    print(f\"   UEè¨­å‚™æ•¸é‡: {ue_data_full['ue_id'].nunique()}\")\n",
        "\n",
        "if slice_data_full is not None:\n",
        "    print(f\"ðŸ° åˆ‡ç‰‡è³‡æ–™: {len(slice_data_full):,} ç­†è¨˜éŒ„\")\n",
        "    print(f\"   IMSIæ•¸é‡: {slice_data_full['imsi'].nunique()}\")\n",
        "\n",
        "print(\"\\nðŸš€ è³‡æ–™é›†å·²æº–å‚™å®Œæˆï¼Œå¯é€²è¡Œå¾ŒçºŒåˆ†æžï¼\")\n",
        "\n",
        "# ===== åœ¨ Cell 2 æœ€å¾ŒåŠ å…¥ä»¥ä¸‹ç¨‹å¼ç¢¼ =====\n",
        "\n",
        "# ä¿å­˜åŽŸå§‹è¼‰å…¥çš„è³‡æ–™ç‚º parquet æª”æ¡ˆ\n",
        "def save_raw_data_to_parquet():\n",
        "    \"\"\"å°‡è¼‰å…¥çš„åŽŸå§‹è³‡æ–™ä¿å­˜ç‚º parquet æª”æ¡ˆ\"\"\"\n",
        "    print(\"\\n\" + \"ðŸ’¾\" + \"=\"*78 + \"ðŸ’¾\")\n",
        "    print(\"ä¿å­˜åŽŸå§‹è³‡æ–™ç‚º Parquet æª”æ¡ˆ\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    saved_files = []\n",
        "\n",
        "    # ä¿å­˜åŸºç«™è³‡æ–™\n",
        "    if bs_data_full is not None and len(bs_data_full) > 0:\n",
        "        bs_filename = 'raw_bs_data.parquet'\n",
        "        bs_data_full.to_parquet(bs_filename, compression='snappy', index=False)\n",
        "        file_size = os.path.getsize(bs_filename) / 1024 / 1024\n",
        "        print(f\"âœ… åŸºç«™è³‡æ–™å·²ä¿å­˜: {bs_filename} ({file_size:.1f} MB)\")\n",
        "        saved_files.append(bs_filename)\n",
        "\n",
        "    # ä¿å­˜ UE è³‡æ–™\n",
        "    if ue_data_full is not None and len(ue_data_full) > 0:\n",
        "        ue_filename = 'raw_ue_data.parquet'\n",
        "        ue_data_full.to_parquet(ue_filename, compression='snappy', index=False)\n",
        "        file_size = os.path.getsize(ue_filename) / 1024 / 1024\n",
        "        print(f\"âœ… UEè³‡æ–™å·²ä¿å­˜: {ue_filename} ({file_size:.1f} MB)\")\n",
        "        saved_files.append(ue_filename)\n",
        "\n",
        "    # ä¿å­˜åˆ‡ç‰‡è³‡æ–™\n",
        "    if slice_data_full is not None and len(slice_data_full) > 0:\n",
        "        slice_filename = 'raw_slice_data.parquet'\n",
        "        slice_data_full.to_parquet(slice_filename, compression='snappy', index=False)\n",
        "        file_size = os.path.getsize(slice_filename) / 1024 / 1024\n",
        "        print(f\"âœ… åˆ‡ç‰‡è³‡æ–™å·²ä¿å­˜: {slice_filename} ({file_size:.1f} MB)\")\n",
        "        saved_files.append(slice_filename)\n",
        "\n",
        "    # ä¿å­˜åˆ‡ç‰‡é…ç½®è³‡è¨Š\n",
        "    import json\n",
        "    config_filename = 'slice_configs.json'\n",
        "    with open(config_filename, 'w') as f:\n",
        "        json.dump(processor_pro.slice_configs, f, indent=2)\n",
        "    print(f\"âœ… åˆ‡ç‰‡é…ç½®å·²ä¿å­˜: {config_filename}\")\n",
        "    saved_files.append(config_filename)\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ æ‰€æœ‰åŽŸå§‹è³‡æ–™å·²æˆåŠŸä¿å­˜ï¼\")\n",
        "    print(f\"ðŸ“ ä¿å­˜æª”æ¡ˆæ¸…å–®: {saved_files}\")\n",
        "    print(f\"ðŸ’¡ ä¸‹æ¬¡å¯ç›´æŽ¥å¾ž Cell 3 é–‹å§‹åŸ·è¡Œï¼Œè‡ªå‹•è¼‰å…¥é€™äº›æª”æ¡ˆ\")\n",
        "\n",
        "    return saved_files\n",
        "\n",
        "# åŸ·è¡Œä¿å­˜\n",
        "saved_files = save_raw_data_to_parquet()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vFDUl3XT9-7"
      },
      "source": [
        "### 3. è³‡æ–™å‰è™•ç†èˆ‡ç‰¹å¾µå·¥ç¨‹"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TensorFlow GPU åŠ é€Ÿç‰ˆè¨˜æ†¶é«”å„ªåŒ–ç‰¹å¾µå·¥ç¨‹è™•ç†å™¨\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "from datetime import datetime\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# æª¢æŸ¥ GPU å¯ç”¨æ€§ä¸¦è¨­å®š\n",
        "print(\"ðŸ” æª¢æŸ¥ GPU ç’°å¢ƒ...\")\n",
        "print(f\"TensorFlow ç‰ˆæœ¬: {tf.__version__}\")\n",
        "print(f\"GPU å¯ç”¨: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"CUDA æ”¯æ´: {tf.test.is_built_with_cuda()}\")\n",
        "\n",
        "# è¨­å®š GPU è¨˜æ†¶é«”å¢žé•·\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"âœ… GPU è¨˜æ†¶é«”å¢žé•·å·²å•Ÿç”¨\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"âš ï¸ GPU è¨­å®šè­¦å‘Š: {e}\")\n",
        "\n",
        "def load_raw_data_if_exists():\n",
        "    \"\"\"è¼‰å…¥åŽŸå§‹è³‡æ–™\"\"\"\n",
        "    print(\"ðŸ” æª¢æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿å­˜çš„åŽŸå§‹è³‡æ–™...\")\n",
        "\n",
        "    required_files = [\n",
        "        'raw_bs_data.parquet',\n",
        "        'raw_ue_data.parquet',\n",
        "        'raw_slice_data.parquet',\n",
        "        'slice_configs.json'\n",
        "    ]\n",
        "\n",
        "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"âŒ ç¼ºå°‘æª”æ¡ˆ: {missing_files}\")\n",
        "        print(\"ðŸ’¡ è«‹å…ˆåŸ·è¡Œ Cell 2 è¼‰å…¥åŽŸå§‹è³‡æ–™\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    print(\"âœ… æ‰¾åˆ°æ‰€æœ‰å¿…è¦æª”æ¡ˆï¼Œé–‹å§‹è¼‰å…¥...\")\n",
        "\n",
        "    try:\n",
        "        bs_data = pd.read_parquet('raw_bs_data.parquet')\n",
        "        ue_data = pd.read_parquet('raw_ue_data.parquet')\n",
        "        slice_data = pd.read_parquet('raw_slice_data.parquet')\n",
        "\n",
        "        with open('slice_configs.json', 'r') as f:\n",
        "            slice_configs = json.load(f)\n",
        "\n",
        "        print(f\"âœ… è³‡æ–™è¼‰å…¥å®Œæˆï¼\")\n",
        "        print(f\" ðŸ“Š åŸºç«™è³‡æ–™: {len(bs_data):,} ç­†è¨˜éŒ„\")\n",
        "        print(f\" ðŸ“± UEè³‡æ–™: {len(ue_data):,} ç­†è¨˜éŒ„\")\n",
        "        print(f\" ðŸ° åˆ‡ç‰‡è³‡æ–™: {len(slice_data):,} ç­†è¨˜éŒ„\")\n",
        "\n",
        "        return bs_data, ue_data, slice_data, slice_configs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ è¼‰å…¥è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "class TensorFlowGPUEnhancedProcessor:\n",
        "    \"\"\"TensorFlow GPU åŠ é€Ÿå¢žå¼·ç‰ˆç‰¹å¾µå·¥ç¨‹è™•ç†å™¨\"\"\"\n",
        "\n",
        "    def __init__(self, slice_configs, batch_size=250000):\n",
        "        self.slice_configs = slice_configs\n",
        "        self.batch_size = batch_size\n",
        "        self.gpu_available = len(tf.config.list_physical_devices('GPU')) > 0\n",
        "\n",
        "        print(f\"ðŸš€ åˆå§‹åŒ– TensorFlow GPU åŠ é€Ÿè™•ç†å™¨\")\n",
        "        print(f\"ðŸ“¦ æ‰¹æ¬¡å¤§å°: {self.batch_size:,} ç­†è¨˜éŒ„\")\n",
        "        print(f\"ðŸŽ® GPU æ¨¡å¼: {'å•Ÿç”¨' if self.gpu_available else 'ç¦ç”¨ï¼ˆä½¿ç”¨CPUï¼‰'}\")\n",
        "\n",
        "    def process_data_in_batches(self, slice_data, ue_data, bs_data):\n",
        "        \"\"\"TensorFlow GPU åŠ é€Ÿåˆ†æ‰¹è™•ç†\"\"\"\n",
        "        print(f\"ðŸš€ é–‹å§‹ TensorFlow GPU åˆ†æ‰¹è™•ç†ï¼Œç¸½è¨˜éŒ„æ•¸: {len(slice_data):,}\")\n",
        "\n",
        "        # é è™•ç†è¼”åŠ©è³‡æ–™\n",
        "        ue_data_processed = self._preprocess_ue_data(ue_data)\n",
        "        bs_data_processed = self._preprocess_bs_data(bs_data)\n",
        "\n",
        "        total_rows = len(slice_data)\n",
        "        num_batches = (total_rows + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "        processed_results = []\n",
        "\n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * self.batch_size\n",
        "            end_idx = min((batch_idx + 1) * self.batch_size, total_rows)\n",
        "\n",
        "            print(f\" ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ {batch_idx + 1}/{num_batches} ({start_idx:,}-{end_idx:,})\")\n",
        "\n",
        "            batch_data = slice_data.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "            # TensorFlow GPU æ‰¹æ¬¡è™•ç†\n",
        "            processed_batch = self._process_single_batch_tensorflow(\n",
        "                batch_data, ue_data_processed, bs_data_processed\n",
        "            )\n",
        "\n",
        "            if processed_batch is not None:\n",
        "                processed_results.append(processed_batch)\n",
        "                print(f\" âœ… æ‰¹æ¬¡å®Œæˆ: {len(processed_batch):,} ç­†è¨˜éŒ„\")\n",
        "\n",
        "            # è¨˜æ†¶é«”æ¸…ç†\n",
        "            del batch_data, processed_batch\n",
        "            gc.collect()\n",
        "\n",
        "            if (batch_idx + 1) % 5 == 0:\n",
        "                print(f\" ðŸ’¾ å·²è™•ç† {batch_idx + 1} æ‰¹æ¬¡\")\n",
        "\n",
        "        # åˆä½µçµæžœ\n",
        "        if processed_results:\n",
        "            print(\"ðŸ”— åˆä½µæ‰€æœ‰æ‰¹æ¬¡çµæžœ...\")\n",
        "            final_result = pd.concat(processed_results, ignore_index=True)\n",
        "\n",
        "            del processed_results\n",
        "            gc.collect()\n",
        "\n",
        "            return final_result\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _preprocess_ue_data(self, ue_data):\n",
        "        \"\"\"é è™•ç† UE è³‡æ–™\"\"\"\n",
        "        if ue_data is None or len(ue_data) == 0:\n",
        "            return None\n",
        "\n",
        "        ue_processed = ue_data.copy()\n",
        "        ue_processed['time_ms'] = ue_processed['time']\n",
        "\n",
        "        ue_features = ['time_ms', 'bs_id', 'ue_id', 'rsrp', 'pl', 'dl_snr', 'dl_bler', 'ul_bler']\n",
        "        available_ue_features = [col for col in ue_features if col in ue_processed.columns]\n",
        "\n",
        "        return ue_processed[available_ue_features]\n",
        "\n",
        "    def _preprocess_bs_data(self, bs_data):\n",
        "        \"\"\"é è™•ç†åŸºç«™è³‡æ–™\"\"\"\n",
        "        if bs_data is None or len(bs_data) == 0:\n",
        "            return None\n",
        "\n",
        "        bs_processed = bs_data.copy()\n",
        "        bs_processed['time_ms'] = bs_processed['time']\n",
        "\n",
        "        bs_features = ['time_ms', 'bs_id', 'nof_ue', 'dl_brate', 'ul_brate']\n",
        "        available_bs_features = [col for col in bs_features if col in bs_processed.columns]\n",
        "\n",
        "        return bs_processed[available_bs_features]\n",
        "\n",
        "    def _process_single_batch_tensorflow(self, df, ue_data, bs_data):\n",
        "        \"\"\"TensorFlow GPU åŠ é€Ÿç‰ˆå–®æ‰¹æ¬¡è™•ç†\"\"\"\n",
        "        try:\n",
        "            # ä½¿ç”¨ TensorFlow GPU è¨­å‚™\n",
        "            device = '/GPU:0' if self.gpu_available else '/CPU:0'\n",
        "\n",
        "            with tf.device(device):\n",
        "                # 1. æ™‚é–“ç‰¹å¾µï¼ˆTensorFlow å‘é‡åŒ–ï¼‰\n",
        "                if 'Timestamp' in df.columns:\n",
        "                    timestamps = pd.to_datetime(df['Timestamp'], unit='ms', errors='coerce')\n",
        "                    df['hour'] = timestamps.dt.hour.astype('uint8')\n",
        "                    df['minute'] = timestamps.dt.minute.astype('uint8')\n",
        "                    df['day_of_week'] = timestamps.dt.dayofweek.astype('uint8')\n",
        "                    del timestamps\n",
        "\n",
        "                # 2. æŽ’ç¨‹ç­–ç•¥ç·¨ç¢¼\n",
        "                sched_mapping = {'sched0': 0, 'sched1': 1, 'sched2': 2}\n",
        "                df['sched_policy_num'] = df['sched_policy'].map(sched_mapping).fillna(0).astype('uint8')\n",
        "\n",
        "                # 3. RBG é…ç½®\n",
        "                df['allocated_rbgs'] = self._vectorized_rbg_allocation(df)\n",
        "\n",
        "                # 4. åŸºæœ¬è³‡æºåˆ©ç”¨çŽ‡ï¼ˆTensorFlow GPU å‘é‡åŒ–ï¼‰\n",
        "                df['sum_requested_prbs'] = df.get('sum_requested_prbs', pd.Series([0]*len(df))).fillna(0)\n",
        "                df['sum_granted_prbs'] = df.get('sum_granted_prbs', pd.Series([0]*len(df))).fillna(0)\n",
        "\n",
        "                # è½‰æ›ç‚º TensorFlow tensors é€²è¡Œ GPU é‹ç®—\n",
        "                requested = tf.constant(df['sum_requested_prbs'].values, dtype=tf.float32)\n",
        "                granted = tf.constant(df['sum_granted_prbs'].values, dtype=tf.float32)\n",
        "\n",
        "                # GPU æ¢ä»¶é‹ç®—\n",
        "                prb_utilization = tf.where(\n",
        "                    requested > 0,\n",
        "                    granted / requested,\n",
        "                    0.0\n",
        "                )\n",
        "                df['prb_utilization'] = tf.clip_by_value(prb_utilization, 0.0, 1.0).numpy()\n",
        "\n",
        "                # 5. PRB ä¾›éœ€å£“åŠ›ç‰¹å¾µï¼ˆTensorFlow GPUï¼‰\n",
        "                slice_prb = df.get('slice_prb', pd.Series([1]*len(df))).fillna(1)\n",
        "                slice_prb_tensor = tf.constant(slice_prb.values, dtype=tf.float32)\n",
        "\n",
        "                prb_demand_pressure = tf.where(\n",
        "                    slice_prb_tensor > 0,\n",
        "                    requested / slice_prb_tensor,\n",
        "                    0.0\n",
        "                )\n",
        "                df['prb_demand_pressure'] = tf.clip_by_value(prb_demand_pressure, 0.0, 5.0).numpy()\n",
        "\n",
        "                # 6. HARQ é‡å‚³çŽ‡ï¼ˆTensorFlow GPUï¼‰\n",
        "                tx_pkts_col = 'tx_pkts downlink'\n",
        "                tx_errors_col = 'tx_errors downlink (%)'\n",
        "\n",
        "                if tx_pkts_col in df.columns and tx_errors_col in df.columns:\n",
        "                    tx_pkts = tf.constant(df[tx_pkts_col].fillna(0).values, dtype=tf.float32)\n",
        "                    tx_errors = tf.constant(df[tx_errors_col].fillna(0).values, dtype=tf.float32)\n",
        "\n",
        "                    harq_rate = tf.where(\n",
        "                        tx_pkts > 0,\n",
        "                        tx_errors / 100.0,\n",
        "                        0.0\n",
        "                    )\n",
        "                    df['harq_retransmission_rate'] = tf.clip_by_value(harq_rate, 0.0, 1.0).numpy()\n",
        "                else:\n",
        "                    df['harq_retransmission_rate'] = np.zeros(len(df), dtype='float32')\n",
        "\n",
        "                # 7. åžåé‡æ•ˆçŽ‡ï¼ˆTensorFlow GPUï¼‰\n",
        "                throughput_col = 'tx_brate downlink [Mbps]'\n",
        "                ul_throughput_col = 'rx_brate uplink [Mbps]'\n",
        "\n",
        "                if throughput_col in df.columns:\n",
        "                    dl_throughput = tf.constant(df[throughput_col].fillna(0).values, dtype=tf.float32)\n",
        "                    dl_efficiency = tf.where(\n",
        "                        granted > 0,\n",
        "                        dl_throughput / granted,\n",
        "                        0.0\n",
        "                    )\n",
        "                    df['dl_throughput_efficiency'] = dl_efficiency.numpy()\n",
        "                else:\n",
        "                    df['dl_throughput_efficiency'] = np.zeros(len(df), dtype='float32')\n",
        "\n",
        "                if ul_throughput_col in df.columns:\n",
        "                    ul_throughput = tf.constant(df[ul_throughput_col].fillna(0).values, dtype=tf.float32)\n",
        "                    ul_efficiency = tf.where(\n",
        "                        granted > 0,\n",
        "                        ul_throughput / granted,\n",
        "                        0.0\n",
        "                    )\n",
        "                    df['ul_throughput_efficiency'] = ul_efficiency.numpy()\n",
        "                else:\n",
        "                    df['ul_throughput_efficiency'] = np.zeros(len(df), dtype='float32')\n",
        "\n",
        "                # 8. ä¸Šä¸‹è¡Œå°ç¨±æ€§ï¼ˆTensorFlow GPUï¼‰\n",
        "                dl_eff_tensor = tf.constant(df['dl_throughput_efficiency'].values, dtype=tf.float32)\n",
        "                ul_eff_tensor = tf.constant(df['ul_throughput_efficiency'].values, dtype=tf.float32)\n",
        "\n",
        "                throughput_symmetry = tf.where(\n",
        "                    dl_eff_tensor > 0,\n",
        "                    ul_eff_tensor / (dl_eff_tensor + 1e-6),\n",
        "                    0.0\n",
        "                )\n",
        "                df['throughput_symmetry'] = tf.clip_by_value(throughput_symmetry, 0.0, 10.0).numpy()\n",
        "\n",
        "                # 9. æŽ’ç¨‹ç­‰å¾…æ™‚é–“\n",
        "                df['scheduling_wait_time'] = self._calculate_scheduling_wait_time(df)\n",
        "\n",
        "                # 10. åˆä½µ UE ç‰¹å¾µ\n",
        "                df = self._merge_ue_features_tensorflow(df, ue_data)\n",
        "\n",
        "                # 11. å¢žå¼·ç‰ˆ QoS è©•åˆ†ï¼ˆTensorFlow GPUï¼‰\n",
        "                df['qos_score'] = self._calculate_enhanced_qos_score_tensorflow(df)\n",
        "\n",
        "                # 12. ç¶²è·¯è² è¼‰\n",
        "                df['num_ues'] = df.get('num_ues', pd.Series([1]*len(df))).fillna(1)\n",
        "                df['network_load'] = df['num_ues'] / 42.0\n",
        "\n",
        "                # 13. æ”¹é€²çš„ç¶œåˆæ•ˆçŽ‡æŒ‡æ¨™ï¼ˆTensorFlow GPUï¼‰\n",
        "                df['allocation_efficiency'] = self._calculate_improved_allocation_efficiency_tensorflow(df)\n",
        "\n",
        "                # 14. é¸æ“‡æœ€çµ‚ç‰¹å¾µ\n",
        "                required_columns = [\n",
        "                    'num_ues', 'slice_id', 'sched_policy_num', 'allocated_rbgs',\n",
        "                    'bs_id', 'exp_id', 'sum_requested_prbs', 'sum_granted_prbs',\n",
        "                    'prb_utilization', 'dl_throughput_efficiency', 'ul_throughput_efficiency',\n",
        "                    'qos_score', 'network_load', 'hour', 'minute', 'day_of_week',\n",
        "                    'prb_demand_pressure', 'harq_retransmission_rate', 'throughput_symmetry',\n",
        "                    'scheduling_wait_time', 'sinr_analog', 'sinr_category',\n",
        "                    'allocation_efficiency', 'sched_policy', 'training_config'\n",
        "                ]\n",
        "\n",
        "                available_columns = [col for col in required_columns if col in df.columns]\n",
        "                df_result = df[available_columns].copy()\n",
        "\n",
        "                # æ¸…ç†ç•°å¸¸å€¼\n",
        "                df_result = df_result.dropna(subset=['allocation_efficiency'])\n",
        "\n",
        "                return df_result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" âŒ TensorFlow æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _vectorized_rbg_allocation(self, df):\n",
        "        \"\"\"å‘é‡åŒ– RBG é…ç½®è¨ˆç®—\"\"\"\n",
        "        config_map = {}\n",
        "        for config_name, rbg_list in self.slice_configs.items():\n",
        "            for slice_id in range(len(rbg_list)):\n",
        "                config_map[(config_name, slice_id)] = rbg_list[slice_id]\n",
        "\n",
        "        allocation_result = []\n",
        "        for idx, row in df.iterrows():\n",
        "            training_config = row.get('training_config', 'tr0')\n",
        "            slice_id = row.get('slice_id', 0)\n",
        "            key = (training_config, slice_id)\n",
        "            allocation_result.append(config_map.get(key, 0))\n",
        "\n",
        "        return pd.Series(allocation_result, dtype='uint8')\n",
        "\n",
        "    def _calculate_scheduling_wait_time(self, df):\n",
        "        \"\"\"è¨ˆç®—æŽ’ç¨‹ç­‰å¾…æ™‚é–“\"\"\"\n",
        "        wait_times = []\n",
        "\n",
        "        for _, group in df.groupby(['bs_id', 'IMSI']):\n",
        "            granted_prbs = group['sum_granted_prbs'].values\n",
        "            wait_time = 0\n",
        "            current_wait = 0\n",
        "\n",
        "            for prb in granted_prbs:\n",
        "                if prb == 0:\n",
        "                    current_wait += 1\n",
        "                else:\n",
        "                    wait_time = max(wait_time, current_wait)\n",
        "                    current_wait = 0\n",
        "\n",
        "            wait_time = max(wait_time, current_wait)\n",
        "            wait_times.extend([wait_time] * len(group))\n",
        "\n",
        "        return pd.Series(wait_times, index=df.index, dtype='float32')\n",
        "\n",
        "    def _merge_ue_features_tensorflow(self, df, ue_data):\n",
        "        \"\"\"TensorFlow ç‰ˆæœ¬çš„ UE ç‰¹å¾µåˆä½µ\"\"\"\n",
        "        if ue_data is None or len(ue_data) == 0:\n",
        "            df['sinr_analog'] = np.zeros(len(df), dtype='float32')\n",
        "            df['sinr_category'] = np.zeros(len(df), dtype='uint8')\n",
        "            return df\n",
        "\n",
        "        # ç°¡åŒ–çš„ SINR é¡žæ¯”è¨ˆç®—\n",
        "        sinr_analog_values = []\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            # ä½¿ç”¨ç°¡åŒ–æ–¹æ³•æˆ–å›ºå®šå€¼\n",
        "            sinr_analog_values.append(0.0)\n",
        "\n",
        "        df['sinr_analog'] = pd.Series(sinr_analog_values, dtype='float32')\n",
        "\n",
        "        # TensorFlow GPU åˆ†é¡ž\n",
        "        with tf.device('/GPU:0' if self.gpu_available else '/CPU:0'):\n",
        "            sinr_tensor = tf.constant(df['sinr_analog'].values, dtype=tf.float32)\n",
        "\n",
        "            # ä½¿ç”¨ TensorFlow é€²è¡Œåˆ†é¡ž\n",
        "            sinr_category = tf.cast(\n",
        "                tf.clip_by_value(\n",
        "                    tf.floor((sinr_tensor + 10) / 5),\n",
        "                    0, 4\n",
        "                ), tf.uint8\n",
        "            )\n",
        "            df['sinr_category'] = sinr_category.numpy()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _calculate_enhanced_qos_score_tensorflow(self, df):\n",
        "        \"\"\"TensorFlow GPU ç‰ˆæœ¬çš„å¢žå¼· QoS è©•åˆ†\"\"\"\n",
        "        with tf.device('/GPU:0' if self.gpu_available else '/CPU:0'):\n",
        "            # 1. HARQ è©•åˆ†\n",
        "            harq_tensor = tf.constant(df['harq_retransmission_rate'].values, dtype=tf.float32)\n",
        "            harq_score = 1.0 - harq_tensor\n",
        "\n",
        "            # 2. CQI è©•åˆ†\n",
        "            cqi_col = 'dl_cqi'\n",
        "            if cqi_col in df.columns:\n",
        "                cqi_tensor = tf.constant(df[cqi_col].fillna(7.5).values, dtype=tf.float32)\n",
        "                cqi_score = cqi_tensor / 15.0\n",
        "            else:\n",
        "                cqi_score = tf.constant(0.5, dtype=tf.float32, shape=(len(df),))\n",
        "\n",
        "            # 3. å»¶é²è©•åˆ†ï¼ˆåŸºæ–¼æŽ’ç¨‹ç­‰å¾…æ™‚é–“ï¼‰\n",
        "            wait_tensor = tf.constant(df['scheduling_wait_time'].values, dtype=tf.float32)\n",
        "            delay_score = tf.where(\n",
        "                wait_tensor > 0,\n",
        "                1.0 / (1.0 + wait_tensor),\n",
        "                1.0\n",
        "            )\n",
        "\n",
        "            # 4. ç¶œåˆè©•åˆ†\n",
        "            qos_score = (\n",
        "                0.4 * harq_score +\n",
        "                0.3 * cqi_score +\n",
        "                0.3 * delay_score\n",
        "            )\n",
        "\n",
        "            return tf.clip_by_value(qos_score, 0.0, 1.0).numpy()\n",
        "\n",
        "    def _calculate_improved_allocation_efficiency_tensorflow(self, df):\n",
        "        \"\"\"TensorFlow GPU ç‰ˆæœ¬çš„æ”¹é€²åˆ†é…æ•ˆçŽ‡è¨ˆç®—\"\"\"\n",
        "        with tf.device('/GPU:0' if self.gpu_available else '/CPU:0'):\n",
        "            # æ‰€æœ‰æŒ‡æ¨™è½‰ç‚º TensorFlow tensors\n",
        "            harq_tensor = tf.constant(df['harq_retransmission_rate'].values, dtype=tf.float32)\n",
        "            demand_tensor = tf.constant(df['prb_demand_pressure'].values, dtype=tf.float32)\n",
        "            wait_tensor = tf.constant(df['scheduling_wait_time'].values, dtype=tf.float32)\n",
        "            sinr_tensor = tf.constant(df['sinr_analog'].values, dtype=tf.float32)\n",
        "\n",
        "            # ä¸¦è¡Œè¨ˆç®—æ‰€æœ‰è©•åˆ†\n",
        "            quality_score = 1.0 - harq_tensor\n",
        "\n",
        "            resource_efficiency = tf.where(\n",
        "                demand_tensor > 0,\n",
        "                1.0 / (1.0 + demand_tensor),\n",
        "                1.0\n",
        "            )\n",
        "\n",
        "            fairness_score = tf.where(\n",
        "                wait_tensor > 0,\n",
        "                1.0 / (1.0 + wait_tensor / 10.0),\n",
        "                1.0\n",
        "            )\n",
        "\n",
        "            radio_quality = tf.where(\n",
        "                sinr_tensor > 0,\n",
        "                tf.nn.tanh(sinr_tensor / 20.0),\n",
        "                0.5\n",
        "            )\n",
        "\n",
        "            # GPU å‘é‡åŒ–ç¶œåˆè¨ˆç®—\n",
        "            allocation_efficiency = (\n",
        "                0.3 * quality_score +\n",
        "                0.3 * resource_efficiency +\n",
        "                0.2 * fairness_score +\n",
        "                0.2 * radio_quality\n",
        "            )\n",
        "\n",
        "            return tf.clip_by_value(allocation_efficiency, 0.0, 1.0).numpy()\n",
        "\n",
        "def optimize_datatypes_tensorflow(df):\n",
        "    \"\"\"TensorFlow ç‰ˆæœ¬çš„è³‡æ–™åž‹åˆ¥æœ€ä½³åŒ–\"\"\"\n",
        "    print(\"ðŸ”§ TensorFlow è³‡æ–™åž‹åˆ¥æœ€ä½³åŒ–...\")\n",
        "\n",
        "    initial_memory = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
        "\n",
        "    # æ•´æ•¸åž‹åˆ¥æœ€ä½³åŒ–\n",
        "    int_cols = df.select_dtypes(include=['int64']).columns\n",
        "    for col in int_cols:\n",
        "        col_min, col_max = df[col].min(), df[col].max()\n",
        "        if col_min >= 0 and col_max < 255:\n",
        "            df[col] = df[col].astype('uint8')\n",
        "        elif col_min >= 0 and col_max < 65535:\n",
        "            df[col] = df[col].astype('uint16')\n",
        "        elif col_min >= -128 and col_max < 127:\n",
        "            df[col] = df[col].astype('int8')\n",
        "        elif col_min >= -32768 and col_max < 32767:\n",
        "            df[col] = df[col].astype('int16')\n",
        "\n",
        "    # æµ®é»žæ•¸åž‹åˆ¥æœ€ä½³åŒ–\n",
        "    float_cols = df.select_dtypes(include=['float64']).columns\n",
        "    for col in float_cols:\n",
        "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
        "\n",
        "    # é¡žåˆ¥åž‹è³‡æ–™\n",
        "    object_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in object_cols:\n",
        "        if df[col].nunique() / len(df) < 0.5:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    final_memory = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
        "    print(f\" ðŸ’¾ è¨˜æ†¶é«”æœ€ä½³åŒ–: {initial_memory:.1f} MB â†’ {final_memory:.1f} MB\")\n",
        "    print(f\" ðŸ“‰ ç¯€çœ: {((initial_memory - final_memory) / initial_memory * 100):.1f}%\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_processed_data_to_parquet_tensorflow(processed_data, feature_names):\n",
        "    \"\"\"TensorFlow ç‰ˆæœ¬çš„è³‡æ–™å„²å­˜\"\"\"\n",
        "    if processed_data is None or len(processed_data) == 0:\n",
        "        print(\"âŒ æ²’æœ‰è³‡æ–™å¯å„²å­˜\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nðŸ’¾ TensorFlow è™•ç†è³‡æ–™å„²å­˜ä¸­...\")\n",
        "\n",
        "    # è³‡æ–™åž‹åˆ¥æœ€ä½³åŒ–\n",
        "    optimized_data = optimize_datatypes_tensorflow(processed_data.copy())\n",
        "\n",
        "    # å„²å­˜ç‚º Parquet\n",
        "    output_filename = 'coloran_processed_features_tensorflow.parquet'\n",
        "    optimized_data.to_parquet(\n",
        "        output_filename,\n",
        "        compression='snappy',\n",
        "        index=False,\n",
        "        engine='pyarrow'\n",
        "    )\n",
        "\n",
        "    file_size = os.path.getsize(output_filename) / 1024 / 1024\n",
        "    memory_size = optimized_data.memory_usage(deep=True).sum() / 1024 / 1024\n",
        "\n",
        "    print(f\"âœ… TensorFlow å¢žå¼·ç‰¹å¾µè³‡æ–™å·²å„²å­˜: {output_filename}\")\n",
        "    print(f\"ðŸ“Š è¨˜éŒ„æ•¸é‡: {len(optimized_data):,}\")\n",
        "    print(f\"ðŸ“‹ ç‰¹å¾µæ•¸é‡: {len(feature_names)}\")\n",
        "    print(f\"ðŸ’¾ æª”æ¡ˆå¤§å°: {file_size:.1f} MB\")\n",
        "    print(f\"ðŸ—œï¸ å£“ç¸®æ•ˆçŽ‡: {memory_size/file_size:.1f}x\")\n",
        "\n",
        "    # å„²å­˜ç‰¹å¾µå…ƒè³‡æ–™\n",
        "    feature_info = {\n",
        "        'feature_names': feature_names,\n",
        "        'total_records': len(optimized_data),\n",
        "        'processing_date': datetime.now().isoformat(),\n",
        "        'file_size_mb': file_size,\n",
        "        'compression_ratio': memory_size/file_size,\n",
        "        'acceleration_type': 'TensorFlow GPU',\n",
        "        'enhancements': [\n",
        "            'TensorFlow GPUå‘é‡åŒ–',\n",
        "            'PRBä¾›éœ€å£“åŠ›',\n",
        "            'HARQé‡å‚³çŽ‡',\n",
        "            'SINRé¡žæ¯”å€¼',\n",
        "            'æŽ’ç¨‹ç­‰å¾…æ™‚é–“',\n",
        "            'ä¸Šä¸‹è¡Œå°ç¨±æ€§',\n",
        "            'æ”¹é€²çš„allocation_efficiency'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    with open('feature_metadata_tensorflow.json', 'w') as f:\n",
        "        json.dump(feature_info, f, indent=2)\n",
        "\n",
        "    print(f\"ðŸ“‹ TensorFlow ç‰¹å¾µå…ƒè³‡æ–™å·²å„²å­˜: feature_metadata_tensorflow.json\")\n",
        "\n",
        "    # æ¸…ç†è¨˜æ†¶é«”\n",
        "    del optimized_data\n",
        "    gc.collect()\n",
        "\n",
        "    return output_filename\n",
        "\n",
        "def main_processing_pipeline_tensorflow():\n",
        "    \"\"\"TensorFlow GPU åŠ é€Ÿç‰ˆä¸»è¦è™•ç†ç®¡é“\"\"\"\n",
        "    print(\"ðŸš€ å•Ÿå‹• TensorFlow GPU åŠ é€Ÿè¨˜æ†¶é«”å„ªåŒ–ç‰¹å¾µå·¥ç¨‹ç®¡é“\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # æª¢æŸ¥ GPU ç‹€æ…‹\n",
        "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "    print(f\"ðŸŽ® åµæ¸¬åˆ° {len(gpu_devices)} å€‹ GPU è£ç½®\")\n",
        "    if gpu_devices:\n",
        "        for i, gpu in enumerate(gpu_devices):\n",
        "            print(f\"   GPU {i}: {gpu.name}\")\n",
        "\n",
        "    # 1. è¼‰å…¥åŽŸå§‹è³‡æ–™\n",
        "    bs_data_loaded, ue_data_loaded, slice_data_loaded, slice_configs_loaded = load_raw_data_if_exists()\n",
        "\n",
        "    if slice_data_loaded is None:\n",
        "        print(\"âš ï¸ å˜—è©¦ä½¿ç”¨ Cell 2 çš„è¨˜æ†¶é«”è®Šæ•¸...\")\n",
        "        if 'bs_data_full' in globals():\n",
        "            bs_data_loaded = bs_data_full\n",
        "            ue_data_loaded = ue_data_full\n",
        "            slice_data_loaded = slice_data_full\n",
        "            slice_configs_loaded = processor_pro.slice_configs\n",
        "            print(\"âœ… æˆåŠŸä½¿ç”¨ Cell 2 çš„è¨˜æ†¶é«”è³‡æ–™\")\n",
        "        else:\n",
        "            print(\"âŒ è«‹å…ˆåŸ·è¡Œ Cell 2 è¼‰å…¥è³‡æ–™\")\n",
        "            return None, None\n",
        "\n",
        "    # 2. åˆå§‹åŒ– TensorFlow è™•ç†å™¨\n",
        "    print(f\"\\nðŸ”§ åˆå§‹åŒ– TensorFlow GPU è™•ç†å™¨...\")\n",
        "    processor = TensorFlowGPUEnhancedProcessor(\n",
        "        slice_configs_loaded,\n",
        "        batch_size=250000  # TensorFlow ç‰ˆæœ¬ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡\n",
        "    )\n",
        "\n",
        "    # 3. TensorFlow GPU æ‰¹æ¬¡è™•ç†\n",
        "    print(f\"\\nðŸ“Š é–‹å§‹ TensorFlow GPU åŠ é€Ÿç‰¹å¾µå·¥ç¨‹...\")\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    processed_data = processor.process_data_in_batches(\n",
        "        slice_data_loaded, ue_data_loaded, bs_data_loaded\n",
        "    )\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    processing_time = (end_time - start_time).total_seconds()\n",
        "\n",
        "    if processed_data is None:\n",
        "        print(\"âŒ TensorFlow ç‰¹å¾µå·¥ç¨‹è™•ç†å¤±æ•—\")\n",
        "        return None, None\n",
        "\n",
        "    # 4. æ¸…ç†è¨˜æ†¶é«”\n",
        "    del bs_data_loaded, ue_data_loaded, slice_data_loaded\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"âœ… TensorFlow GPU åŠ é€Ÿç‰¹å¾µå·¥ç¨‹å®Œæˆ!\")\n",
        "    print(f\"â±ï¸ è™•ç†æ™‚é–“: {processing_time:.2f} ç§’\")\n",
        "    print(f\"ðŸ“Š æœ€çµ‚è¨˜éŒ„æ•¸: {len(processed_data):,}\")\n",
        "    print(f\"ðŸš€ è™•ç†é€Ÿåº¦: {len(processed_data)/processing_time:.0f} è¨˜éŒ„/ç§’\")\n",
        "\n",
        "    # 5. ç‰¹å¾µåˆ—è¡¨\n",
        "    enhanced_features = [\n",
        "        'num_ues', 'slice_id', 'sched_policy_num', 'allocated_rbgs',\n",
        "        'bs_id', 'exp_id', 'sum_requested_prbs', 'sum_granted_prbs',\n",
        "        'prb_utilization', 'dl_throughput_efficiency', 'ul_throughput_efficiency',\n",
        "        'qos_score', 'network_load', 'hour', 'minute', 'day_of_week',\n",
        "        'prb_demand_pressure', 'harq_retransmission_rate', 'throughput_symmetry',\n",
        "        'scheduling_wait_time', 'sinr_analog', 'sinr_category'\n",
        "    ]\n",
        "\n",
        "    available_features = [f for f in enhanced_features if f in processed_data.columns]\n",
        "\n",
        "    print(f\"\\nðŸ“‹ TensorFlow å¢žå¼·ç‰¹å¾µæ¸…å–® ({len(available_features)} å€‹):\")\n",
        "    for i, feature in enumerate(available_features, 1):\n",
        "        print(f\" {i:2d}. {feature}\")\n",
        "\n",
        "    # 6. å„²å­˜è™•ç†å¾Œçš„è³‡æ–™\n",
        "    saved_file = save_processed_data_to_parquet_tensorflow(processed_data, available_features)\n",
        "\n",
        "    # 7. é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š\n",
        "    if 'allocation_efficiency' in processed_data.columns:\n",
        "        print(f\"\\nðŸ“ˆ æ”¹é€²ç›®æ¨™è®Šæ•¸çµ±è¨ˆ:\")\n",
        "        stats = processed_data['allocation_efficiency'].describe()\n",
        "        print(f\" å¹³å‡: {stats['mean']:.4f}\")\n",
        "        print(f\" æ¨™æº–å·®: {stats['std']:.4f}\")\n",
        "        print(f\" ç¯„åœ: {stats['min']:.4f} - {stats['max']:.4f}\")\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ TensorFlow GPU åŠ é€Ÿç‰ˆ Cell 3 åŸ·è¡Œå®Œæˆï¼\")\n",
        "    print(f\"ðŸ“ è¼¸å‡ºæª”æ¡ˆ: {saved_file}\")\n",
        "    print(f\"ðŸš€ ä¸»è¦å„ªåŒ–:\")\n",
        "    print(f\"   1. TensorFlow GPU å‘é‡åŒ–é‹ç®—\")\n",
        "    print(f\"   2. GPU è¨˜æ†¶é«”æœ€ä½³åŒ–ç®¡ç†\")\n",
        "    print(f\"   3. å¤§æ‰¹æ¬¡ä¸¦è¡Œè™•ç†\")\n",
        "    print(f\"   4. æ‰€æœ‰å¢žå¼·ç‰¹å¾µçš„ GPU åŠ é€Ÿ\")\n",
        "    print(f\"   5. é¿å… RAPIDS ä¾è³´å•é¡Œ\")\n",
        "    print(f\"ðŸ’¡ é æœŸåŠ é€Ÿ: 5-15x ç›¸å°æ–¼ CPU ç‰ˆæœ¬\")\n",
        "    print(f\"ðŸ’¡ ä¸‹ä¸€æ­¥: ä½¿ç”¨ coloran_processed_features_tensorflow.parquet é€²è¡Œè¯é‚¦å­¸ç¿’\")\n",
        "\n",
        "    return processed_data, available_features\n",
        "\n",
        "# åŸ·è¡Œ TensorFlow GPU åŠ é€Ÿç‰ˆæœ¬\n",
        "processed_data_tensorflow, feature_names_tensorflow = main_processing_pipeline_tensorflow()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "aLNQ923Ka7Nq",
        "outputId": "5ba03600-1235-49ab-c464-400640fd4e16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” æª¢æŸ¥ GPU ç’°å¢ƒ...\n",
            "TensorFlow ç‰ˆæœ¬: 2.18.0\n",
            "GPU å¯ç”¨: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "CUDA æ”¯æ´: True\n",
            "âœ… GPU è¨˜æ†¶é«”å¢žé•·å·²å•Ÿç”¨\n",
            "ðŸš€ å•Ÿå‹• TensorFlow GPU åŠ é€Ÿè¨˜æ†¶é«”å„ªåŒ–ç‰¹å¾µå·¥ç¨‹ç®¡é“\n",
            "============================================================\n",
            "ðŸŽ® åµæ¸¬åˆ° 1 å€‹ GPU è£ç½®\n",
            "   GPU 0: /physical_device:GPU:0\n",
            "ðŸ” æª¢æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿å­˜çš„åŽŸå§‹è³‡æ–™...\n",
            "âœ… æ‰¾åˆ°æ‰€æœ‰å¿…è¦æª”æ¡ˆï¼Œé–‹å§‹è¼‰å…¥...\n",
            "âœ… è³‡æ–™è¼‰å…¥å®Œæˆï¼\n",
            " ðŸ“Š åŸºç«™è³‡æ–™: 6,534,544 ç­†è¨˜éŒ„\n",
            " ðŸ“± UEè³‡æ–™: 36,974,950 ç­†è¨˜éŒ„\n",
            " ðŸ° åˆ‡ç‰‡è³‡æ–™: 35,512,393 ç­†è¨˜éŒ„\n",
            "\n",
            "ðŸ”§ åˆå§‹åŒ– TensorFlow GPU è™•ç†å™¨...\n",
            "ðŸš€ åˆå§‹åŒ– TensorFlow GPU åŠ é€Ÿè™•ç†å™¨\n",
            "ðŸ“¦ æ‰¹æ¬¡å¤§å°: 250,000 ç­†è¨˜éŒ„\n",
            "ðŸŽ® GPU æ¨¡å¼: å•Ÿç”¨\n",
            "\n",
            "ðŸ“Š é–‹å§‹ TensorFlow GPU åŠ é€Ÿç‰¹å¾µå·¥ç¨‹...\n",
            "ðŸš€ é–‹å§‹ TensorFlow GPU åˆ†æ‰¹è™•ç†ï¼Œç¸½è¨˜éŒ„æ•¸: 35,512,393\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 1/143 (0-250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 2/143 (250,000-500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 3/143 (500,000-750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 4/143 (750,000-1,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 5/143 (1,000,000-1,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 5 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 6/143 (1,250,000-1,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 7/143 (1,500,000-1,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 8/143 (1,750,000-2,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 9/143 (2,000,000-2,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 10/143 (2,250,000-2,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 10 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 11/143 (2,500,000-2,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 12/143 (2,750,000-3,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 13/143 (3,000,000-3,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 14/143 (3,250,000-3,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 15/143 (3,500,000-3,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 15 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 16/143 (3,750,000-4,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 17/143 (4,000,000-4,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 18/143 (4,250,000-4,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 19/143 (4,500,000-4,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 20/143 (4,750,000-5,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 20 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 21/143 (5,000,000-5,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 22/143 (5,250,000-5,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 23/143 (5,500,000-5,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 24/143 (5,750,000-6,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 25/143 (6,000,000-6,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 25 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 26/143 (6,250,000-6,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 27/143 (6,500,000-6,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 28/143 (6,750,000-7,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 29/143 (7,000,000-7,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 30/143 (7,250,000-7,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 30 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 31/143 (7,500,000-7,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 32/143 (7,750,000-8,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 33/143 (8,000,000-8,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 34/143 (8,250,000-8,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 35/143 (8,500,000-8,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 35 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 36/143 (8,750,000-9,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 37/143 (9,000,000-9,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 38/143 (9,250,000-9,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 39/143 (9,500,000-9,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 40/143 (9,750,000-10,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 40 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 41/143 (10,000,000-10,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 42/143 (10,250,000-10,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 43/143 (10,500,000-10,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 44/143 (10,750,000-11,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 45/143 (11,000,000-11,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 45 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 46/143 (11,250,000-11,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 47/143 (11,500,000-11,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 48/143 (11,750,000-12,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 49/143 (12,000,000-12,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 50/143 (12,250,000-12,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 50 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 51/143 (12,500,000-12,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 52/143 (12,750,000-13,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 53/143 (13,000,000-13,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 54/143 (13,250,000-13,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 55/143 (13,500,000-13,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 55 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 56/143 (13,750,000-14,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 57/143 (14,000,000-14,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 58/143 (14,250,000-14,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 59/143 (14,500,000-14,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 60/143 (14,750,000-15,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 60 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 61/143 (15,000,000-15,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 62/143 (15,250,000-15,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 63/143 (15,500,000-15,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 64/143 (15,750,000-16,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 65/143 (16,000,000-16,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 65 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 66/143 (16,250,000-16,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 67/143 (16,500,000-16,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 68/143 (16,750,000-17,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 69/143 (17,000,000-17,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 70/143 (17,250,000-17,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 70 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 71/143 (17,500,000-17,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 72/143 (17,750,000-18,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 73/143 (18,000,000-18,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 74/143 (18,250,000-18,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 75/143 (18,500,000-18,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 75 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 76/143 (18,750,000-19,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 77/143 (19,000,000-19,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 78/143 (19,250,000-19,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 79/143 (19,500,000-19,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 80/143 (19,750,000-20,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 80 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 81/143 (20,000,000-20,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 82/143 (20,250,000-20,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 83/143 (20,500,000-20,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 84/143 (20,750,000-21,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 85/143 (21,000,000-21,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 85 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 86/143 (21,250,000-21,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 87/143 (21,500,000-21,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 88/143 (21,750,000-22,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 89/143 (22,000,000-22,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 90/143 (22,250,000-22,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 90 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 91/143 (22,500,000-22,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 92/143 (22,750,000-23,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 93/143 (23,000,000-23,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 94/143 (23,250,000-23,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 95/143 (23,500,000-23,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 95 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 96/143 (23,750,000-24,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 97/143 (24,000,000-24,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 98/143 (24,250,000-24,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 99/143 (24,500,000-24,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 100/143 (24,750,000-25,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 100 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 101/143 (25,000,000-25,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 102/143 (25,250,000-25,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 103/143 (25,500,000-25,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 104/143 (25,750,000-26,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 105/143 (26,000,000-26,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 105 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 106/143 (26,250,000-26,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 107/143 (26,500,000-26,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 108/143 (26,750,000-27,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 109/143 (27,000,000-27,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 110/143 (27,250,000-27,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 110 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 111/143 (27,500,000-27,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 112/143 (27,750,000-28,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 113/143 (28,000,000-28,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 114/143 (28,250,000-28,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 115/143 (28,500,000-28,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 115 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 116/143 (28,750,000-29,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 117/143 (29,000,000-29,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 118/143 (29,250,000-29,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 119/143 (29,500,000-29,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 120/143 (29,750,000-30,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 120 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 121/143 (30,000,000-30,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 122/143 (30,250,000-30,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 123/143 (30,500,000-30,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 124/143 (30,750,000-31,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 125/143 (31,000,000-31,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 125 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 126/143 (31,250,000-31,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 127/143 (31,500,000-31,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 128/143 (31,750,000-32,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 129/143 (32,000,000-32,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 130/143 (32,250,000-32,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 130 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 131/143 (32,500,000-32,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 132/143 (32,750,000-33,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 133/143 (33,000,000-33,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 134/143 (33,250,000-33,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 135/143 (33,500,000-33,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 135 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 136/143 (33,750,000-34,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 137/143 (34,000,000-34,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 138/143 (34,250,000-34,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 139/143 (34,500,000-34,750,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 140/143 (34,750,000-35,000,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ’¾ å·²è™•ç† 140 æ‰¹æ¬¡\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 141/143 (35,000,000-35,250,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 142/143 (35,250,000-35,500,000)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 250,000 ç­†è¨˜éŒ„\n",
            " ðŸ”¥ TensorFlow è™•ç†æ‰¹æ¬¡ 143/143 (35,500,000-35,512,393)\n",
            " âœ… æ‰¹æ¬¡å®Œæˆ: 12,393 ç­†è¨˜éŒ„\n",
            "ðŸ”— åˆä½µæ‰€æœ‰æ‰¹æ¬¡çµæžœ...\n",
            "âœ… TensorFlow GPU åŠ é€Ÿç‰¹å¾µå·¥ç¨‹å®Œæˆ!\n",
            "â±ï¸ è™•ç†æ™‚é–“: 2653.82 ç§’\n",
            "ðŸ“Š æœ€çµ‚è¨˜éŒ„æ•¸: 35,512,393\n",
            "ðŸš€ è™•ç†é€Ÿåº¦: 13382 è¨˜éŒ„/ç§’\n",
            "\n",
            "ðŸ“‹ TensorFlow å¢žå¼·ç‰¹å¾µæ¸…å–® (22 å€‹):\n",
            "  1. num_ues\n",
            "  2. slice_id\n",
            "  3. sched_policy_num\n",
            "  4. allocated_rbgs\n",
            "  5. bs_id\n",
            "  6. exp_id\n",
            "  7. sum_requested_prbs\n",
            "  8. sum_granted_prbs\n",
            "  9. prb_utilization\n",
            " 10. dl_throughput_efficiency\n",
            " 11. ul_throughput_efficiency\n",
            " 12. qos_score\n",
            " 13. network_load\n",
            " 14. hour\n",
            " 15. minute\n",
            " 16. day_of_week\n",
            " 17. prb_demand_pressure\n",
            " 18. harq_retransmission_rate\n",
            " 19. throughput_symmetry\n",
            " 20. scheduling_wait_time\n",
            " 21. sinr_analog\n",
            " 22. sinr_category\n",
            "\n",
            "ðŸ’¾ TensorFlow è™•ç†è³‡æ–™å„²å­˜ä¸­...\n",
            "ðŸ”§ TensorFlow è³‡æ–™åž‹åˆ¥æœ€ä½³åŒ–...\n",
            " ðŸ’¾ è¨˜æ†¶é«”æœ€ä½³åŒ–: 7879.0 MB â†’ 2133.6 MB\n",
            " ðŸ“‰ ç¯€çœ: 72.9%\n",
            "âœ… TensorFlow å¢žå¼·ç‰¹å¾µè³‡æ–™å·²å„²å­˜: coloran_processed_features_tensorflow.parquet\n",
            "ðŸ“Š è¨˜éŒ„æ•¸é‡: 35,512,393\n",
            "ðŸ“‹ ç‰¹å¾µæ•¸é‡: 22\n",
            "ðŸ’¾ æª”æ¡ˆå¤§å°: 395.7 MB\n",
            "ðŸ—œï¸ å£“ç¸®æ•ˆçŽ‡: 5.4x\n",
            "ðŸ“‹ TensorFlow ç‰¹å¾µå…ƒè³‡æ–™å·²å„²å­˜: feature_metadata_tensorflow.json\n",
            "\n",
            "ðŸ“ˆ æ”¹é€²ç›®æ¨™è®Šæ•¸çµ±è¨ˆ:\n",
            " å¹³å‡: 0.5180\n",
            " æ¨™æº–å·®: 0.1824\n",
            " ç¯„åœ: 0.4580 - 0.7769\n",
            "\n",
            "ðŸŽ‰ TensorFlow GPU åŠ é€Ÿç‰ˆ Cell 3 åŸ·è¡Œå®Œæˆï¼\n",
            "ðŸ“ è¼¸å‡ºæª”æ¡ˆ: coloran_processed_features_tensorflow.parquet\n",
            "ðŸš€ ä¸»è¦å„ªåŒ–:\n",
            "   1. TensorFlow GPU å‘é‡åŒ–é‹ç®—\n",
            "   2. GPU è¨˜æ†¶é«”æœ€ä½³åŒ–ç®¡ç†\n",
            "   3. å¤§æ‰¹æ¬¡ä¸¦è¡Œè™•ç†\n",
            "   4. æ‰€æœ‰å¢žå¼·ç‰¹å¾µçš„ GPU åŠ é€Ÿ\n",
            "   5. é¿å… RAPIDS ä¾è³´å•é¡Œ\n",
            "ðŸ’¡ é æœŸåŠ é€Ÿ: 5-15x ç›¸å°æ–¼ CPU ç‰ˆæœ¬\n",
            "ðŸ’¡ ä¸‹ä¸€æ­¥: ä½¿ç”¨ coloran_processed_features_tensorflow.parquet é€²è¡Œè¯é‚¦å­¸ç¿’\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}